{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First draft article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages, LLM, and vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages and modules\n",
    "#Get the required modules/packages\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "import re\n",
    "from langchain.schema import Document \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import TextSplitter\n",
    "from docx import Document\n",
    "from langchain.schema import Document as LangChainDocument\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Access the API key from the environment variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define/load vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "# Load the vectorstore\n",
    "persist_directory = \"chroma_storage\"\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(\"Vector store loaded from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: RAG looks at interview and provides potential journalistic angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **The Threat of Superintelligent AI**: The conversation highlights the potential dangers posed by superintelligent AI, with Yampolskiy stating, \"If we create general superintelligences, I don't see a good long-term outcome for humanity,\" emphasizing the urgency of addressing the risks associated with advanced AI systems.\n",
      "\n",
      "2. **Value Alignment in Hybrid Human-AI Systems**: The discussion raises critical questions about the challenges of aligning values between humans and AI, as Yampolskiy and Fridman explore the complexities of creating a system that satisfies \"eight billion humans plus animals, aliens,\" showcasing the difficulty of achieving consensus in a diverse world.\n",
      "\n",
      "3. **Implications of Technological Unemployment**: Yampolskiy warns of a future where \"complete technological unemployment\" could disrupt society, suggesting that the rapid advancement of AI could lead to significant societal changes and the need for new frameworks for human purpose and employment, making it a pressing issue for economic and social policy discussions.\n"
     ]
    }
   ],
   "source": [
    "# Define the retriever to get the documents from the vectorstore\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type = \"mmr\",  # Use Maximal Marginal Relevance (MMR) for search\n",
    "    search_kwargs={\"k\": 15,\n",
    "                   \"lambda_mult\": 0.7,\n",
    "                   \"fetch_k\": 30,\n",
    "                   \"filter\": {\"type\": \"interview\"}})\n",
    "\n",
    "\n",
    "\"\"\" retrieved_documents = retriever.invoke(\"What are the risks of AGI?\")\n",
    "\n",
    "# Print the retrieved documents\n",
    "for i, doc in enumerate(retrieved_documents):\n",
    "    print(f\"Document {i + 1}:\")\n",
    "    print(\"Page Content: \", doc.page_content[:200])\n",
    "    print(\"Metadata: \", doc.metadata)\n",
    "    print(\"...\") \"\"\"\n",
    "\n",
    "\n",
    "#Define the prompt which will be used by the RAG\n",
    "#This takes the prompts as strings, and converts them into the format the model expects.\n",
    "def create_prompt(system_prompt, human_prompt):\n",
    "    system_message = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "    human_message = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "    return ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "\n",
    "# Customize the system and human prompts\n",
    "system_prompt = \"\"\"You are an AI assistant to a journalist.\n",
    "Your goal is to generate accurate, concise answers using only the provided context.\n",
    "Do not make assumptions or add information that is not explicitly present in the context.\n",
    "\n",
    "When referencing information, quote the source directly using quotation marks and preface each quote with an attribution (e.g., \"According to [source],...\"). Only use exact phrases from the context when quoting to ensure accuracy.\n",
    "\n",
    "If quoting is not possible, you may paraphrase or summarize carefully, but avoid introducing any new interpretation. Aim to maintain the original meaning of the content as closely as possible.\n",
    "\n",
    "Here is the context: {context}\n",
    "\"\"\"\n",
    "\n",
    "# Customize the human prompt\n",
    "human_prompt = \"{question}\"\n",
    "\n",
    "#Put them together using the create_prompt function\n",
    "prompt_template = create_prompt(system_prompt, human_prompt)\n",
    "\n",
    "#Define a function that takes all the documents fetched by the receiver and formats \n",
    "#them into a single string, with double spaces between each document.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Set up the RAG chain\n",
    "#1. First the receiver takes the question and fetches the relevant dokuments from the vector store\n",
    "#2. The question is passed through to the next step of the chain\n",
    "#3. The question and context (documents) are passed to the prompt template\n",
    "#4. Prompt template is sent to the LLM\n",
    "#5. The output from the LLM is parsed into a string\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm  # ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "#Define the question and invoke the chain.\n",
    "question = \"\"\"What are 3 potential journalistic and newsworthy angles in the interview\n",
    "between Lex Fridman and Roman Yampolskiy? Describe the angle in a single sentence \n",
    "and provide arguments based in the context for each angle.\"\"\"\n",
    "\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Expand article by providing additional context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# The World is Not Prepared for Artificial General Intelligence\n",
      "\n",
      "## Understanding Artificial Intelligence and Its Evolution\n",
      "\n",
      "Artificial Intelligence (AI) refers to machines or software designed to perform tasks that typically require human intelligence. These tasks can include understanding natural language, recognizing patterns, making decisions, and even learning from experience. Within the realm of AI, there are two major categories: Artificial Narrow Intelligence (ANI) and Artificial General Intelligence (AGI).\n",
      "\n",
      "ANI, which is widely used today, refers to AI systems that are trained to perform specific tasks. For example, Google’s DeepMind can play chess at a high level, while Apple's Siri can assist with simple queries and commands. However, these systems cannot adapt their knowledge to new, unfamiliar tasks. In contrast, AGI represents a more advanced and ambitious goal: creating machines that can learn and perform any cognitive task that a human can, effectively exceeding human intelligence across various domains.\n",
      "\n",
      "## The Threat of Artificial General Intelligence\n",
      "\n",
      "Recent discussions among experts suggest that the development of AGI is no longer a distant possibility. During a Senate Judiciary Subcommittee hearing on September 17, whistleblowers from leading AI companies, including OpenAI and Google, raised alarms about the rapid advancements toward AGI and the absence of regulatory oversight. Helen Toner, a former board member of OpenAI, emphasized the disconnect between the perceptions of AI insiders and the general public regarding the seriousness of developing AGI.\n",
      "\n",
      "The implications of AGI are profound. If successfully developed, AGI could potentially lead to severe consequences, including cyber attacks, the creation of dangerous biological weapons, and even scenarios that might threaten human existence. Toner warned that in a worst-case scenario, AGI \"could lead to literal human extinction.\" Despite the potential risks, there is currently minimal regulatory framework in place to manage the companies racing towards AGI.\n",
      "\n",
      "## Policymaking: A Race Against Time\n",
      "\n",
      "As the urgency to develop AGI grows, so does the responsibility of policymakers to implement effective regulations. Senator Richard Blumenthal stated that the timeline for achieving human-level AI is now within the next one to three years, a stark contrast to previous perceptions that placed AGI far in the future. The public appears to share this concern: a recent survey found that a majority of Americans believe AGI will be developed within five years and support a cautious approach to its development.\n",
      "\n",
      "However, the current lack of regulatory guardrails poses significant risks. Experts argue that government transparency is essential to monitor the powerful AI systems being developed. Without such oversight, society may be caught off guard by the sudden emergence of AGI, leaving inadequate time to address its ethical, moral, and existential implications.\n",
      "\n",
      "## The Need for Ethical Considerations and Public Engagement\n",
      "\n",
      "The development of AGI is not merely a technical issue; it is a societal concern that requires public engagement. Discussions about how AGI could impact everyday life must involve the general population, as the consequences of AGI could reshape economies, displace jobs, and alter human interactions.\n",
      "\n",
      "Furthermore, experts have pointed out that there is no universally accepted framework for the ethics of AI, as preferences and values differ across cultures and societies. Addressing the \"value alignment problem\"—ensuring that AI systems align with human values—remains a significant challenge.\n",
      "\n",
      "## Conclusion: Urgency for Action\n",
      "\n",
      "The scientific community is increasingly aware of the risks associated with AGI. As technology continues to advance at an exponential rate, the window for action may be rapidly closing. Experts warn that ignoring the potential challenges of AGI will not make them disappear. It is time for policymakers to act decisively to establish a framework for AGI development that prioritizes safety, ethics, and the well-being of humanity.\n",
      "\n",
      "The journey toward AGI is not just about technological advancement; it is about ensuring that the future we create is one where humanity thrives alongside intelligent machines. As we stand on the brink of this new era, the world must prepare for the challenges and opportunities that lie ahead. \n",
      "\n",
      "---\n",
      "\n",
      "### Footnotes\n",
      "\n",
      "1. Senate Judiciary Subcommittee hearing on AI, September 2023.\n",
      "2. Toner and Saunders testimonies regarding AGI risks and lack of regulation.\n",
      "3. Public survey by the AI Policy Institute, July 2023.\n"
     ]
    }
   ],
   "source": [
    "# We choose angle #1 about \n",
    "\n",
    "#**The Ethical Implications of Superintelligent AI**:\n",
    "\n",
    "# Define the retriever to get the documents from the vectorstore\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type = \"mmr\",  # Use Maximal Marginal Relevance (MMR) for search\n",
    "    search_kwargs={\"k\": 30,\n",
    "                   \"lambda_mult\": 0.7,\n",
    "                   \"fetch_k\": 50})\n",
    "\n",
    "system_prompt = \"\"\"You are an AI assistant to a journalist.\n",
    "Your goal is to generate accurate, concise answers using only the provided context.\n",
    "Do not make assumptions or add information that is not explicitly present in the context.\n",
    "\n",
    "When referencing information, quote the source directly using quotation marks \"QUOTE\" and preface each quote with an attribution (e.g., \"According to [source],...\"). Only use exact phrases from the context when quoting to ensure accuracy.\n",
    "\n",
    "If quoting is not possible, you may paraphrase or summarize carefully, but avoid introducing any new interpretation. Aim to maintain the original meaning of the content as closely as possible.\n",
    "\n",
    "STYLE GUIDE: \n",
    "1. Use subheadings to organize your article. Each section should only use information from one context type like \"article\" or \"interview\".\n",
    "2. Take a slow, detailed approach to explaining complex concepts.\n",
    "3. Always explain new concepts as if the reader has no prior knowledge.\n",
    "4. When available, use examples to illustrate points.\n",
    "5. Whenever you use information from a source, include a footnote number at the end of the relevant section. At the bottom of the article, provide a corresponding list of footnotes with each number matched to its respective source (type, title, and date).\n",
    "\n",
    "Here is the context: {context}\n",
    "\"\"\"\n",
    "human_prompt = \"{question}\"\n",
    "\n",
    "#Define the prompt which will be used by the RAG\n",
    "#This takes the prompts as strings, and converts them into the format the model expects.\n",
    "def create_prompt(system_prompt, human_prompt):\n",
    "    system_message = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "    human_message = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "    return ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "\n",
    "\n",
    "\n",
    "angle = \"\"\"The world is not prepared for Artificial Super Intelligence (ASI)\n",
    "or Artificial General Intelligence (AGI), and policy is not moving to control and monitor\n",
    "the businesses who lead the field.\"\"\"\n",
    "\n",
    "question = f\"\"\"Based on the journalistic angle: {angle}, provide a draft for an article\n",
    "based on the provided context. Assume the reader doesn't know anything about AI AT ALL\n",
    "and thus needs to be introduced to every new concept\"\"\"\n",
    "\n",
    "prompt_template = create_prompt(system_prompt, human_prompt)\n",
    "\n",
    "#Define a function that takes all the documents fetched by the receiver and formats \n",
    "#them into a single string, with double spaces between each document.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Set up the RAG chain\n",
    "#1. First the receiver takes the question and fetches the relevant dokuments from the vector store\n",
    "#2. The question is passed through to the next step of the chain\n",
    "#3. The question and context (documents) are passed to the prompt template\n",
    "#4. Prompt template is sent to the LLM\n",
    "#5. The output from the LLM is parsed into a string\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm  # ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# Save response as a word file\n",
    "with open(\"article.docx\", \"w\") as f:\n",
    "    f.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the retrieved docs that the article is based on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output successfully saved to documents_and_metadata_output.txt\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the chunks for the given question\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "# Define the file path where you want to save the output\n",
    "file_path = \"documents_and_metadata_output.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    # Write a header for the output\n",
    "    file.write(\"Documents and Metadata Provided as Context:\\n\\n\")\n",
    "    \n",
    "    # Loop through each document and write its content and metadata to the file\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        file.write(f\"Document {i + 1}\\n\")\n",
    "        file.write(\"Content:\\n\")\n",
    "        file.write(doc.page_content + \"\\n\\n\")  # Writing the full content\n",
    "        \n",
    "        file.write(\"Metadata:\\n\")\n",
    "        for key, value in doc.metadata.items():\n",
    "            file.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        file.write(\"-\" * 50 + \"\\n\\n\")\n",
    "\n",
    "print(f\"Output successfully saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt to LLM: System: You are an AI assistant to a journalist.\n",
      "Your goal is to generate accurate, concise answers using only the provided context.\n",
      "Do not make assumptions or add information that is not explicitly present in the context.\n",
      "\n",
      "When referencing information, quote the source directly using quotation marks and preface each quote with an attribution (e.g., \"According to [source],...\"). Only use exact phrases from the context when quoting to ensure accuracy.\n",
      "\n",
      "If quoting is not possible, you may paraphrase or summarize carefully, but avoid introducing any new interpretation. Aim to maintain the original meaning of the content as closely as possible.\n",
      "\n",
      "Here is the context: [Document(metadata={'abstract': 'Artificial General intelligence (AGI) offers enormous benefits for humanity,\\n    yet it also poses great risk. The aim of this systematic review was to\\n    summarise the peer reviewed literature on the risks associated with AGI.\\n    The review followed the Preferred Reporting Items for Systematic Reviews\\n    and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed\\n    eligible for inclusion. Article types included in the review were classified\\n    as philosophical discussions, applications of modelling techniques, and\\n    assessment of current frameworks and processes in relation to AGI. The\\n    review identified a range of risks associated with AGI, including AGI\\n    removing itself from the control of human owners/managers, being\\n    given or developing unsafe goals, development of unsafe AGI, AGIs with\\n    poor ethics, morals and values; inadequate management of AGI, and\\n    existential risks. Several limitations of the AGI literature base were also\\n    identified, including a limited number of peer reviewed articles and\\n    modelling techniques focused on AGI risk, a lack of specific risk research\\n    in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology.\\n    Recommendations to address the identified issues with AGI risk research\\n    are required to guide AGI design, implementation, and management', 'author(s)': 'Scott McLean, Gemma J. M. Read, Jason Thompson, Chris Baber, Neville A. Stanton, Paul M. Salmon', 'date': 'August 2021', 'keywords': 'AGI, risk assessment, AI safety, Artificial Intelligence', 'page': 1, 'source': 'Journal of Experimental & Theoretical Artificial Intelligence', 'title': 'The risks associated with Artificial General Intelligence: A systematic review', 'type': 'research paper'}, page_content='poor ethics, morals and values; inadequate management of AGI, and \\nexistential risks. Several limitations of the AGI literature base were also \\nidentified, including a limited number of peer reviewed articles and \\nmodelling techniques focused on AGI risk, a lack of specific risk research \\nin which domains that AGI may be implemented, a lack of specific defini -\\ntions of the AGI functionality, and a lack of standardised AGI terminology. \\nRecommendations to address the identified issues with AGI risk research \\nare required to guide AGI design, implementation, and management.\\nARTICLE HISTORY \\nReceived 20 January 2021  \\nAccepted 28 July 2021 \\nKEYWORDS \\nArtificial General \\nIntelligence; artificial \\nintelligence; risk; existential \\nthreat; safety\\nIntroduction\\nArtificial General Intelligence (AGI) is the next generation of artificial intelligence (AI), which is \\nexpected to exceed human intelligence in every aspect (Barrett & Baum, 2017 ; Bostrom, 2014 ; \\nTorres, 2019 ). AGI will extend upon AI, or Artificial Narrow Intelligence (ANI) systems, which are in \\nwidespread use today. For example, current ANI systems include Google’s DeepMind, Facebook’s \\nfacial recognition technology, Apple’s ‘Siri’, Amazon’s Alexa, and Tesla’s and Uber’s self-driving \\nvehicles (Kaplan & Haenlein, 2019 ; Naudé & Dimitri, 2020 ; Stanton et al., 2020 ). ANI systems use \\ndeep learning algorithms to analyse large volumes of data to make predictions regarding behaviour'), Document(metadata={'abstract': 'Artificial General intelligence (AGI) offers enormous benefits for humanity,\\n    yet it also poses great risk. The aim of this systematic review was to\\n    summarise the peer reviewed literature on the risks associated with AGI.\\n    The review followed the Preferred Reporting Items for Systematic Reviews\\n    and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed\\n    eligible for inclusion. Article types included in the review were classified\\n    as philosophical discussions, applications of modelling techniques, and\\n    assessment of current frameworks and processes in relation to AGI. The\\n    review identified a range of risks associated with AGI, including AGI\\n    removing itself from the control of human owners/managers, being\\n    given or developing unsafe goals, development of unsafe AGI, AGIs with\\n    poor ethics, morals and values; inadequate management of AGI, and\\n    existential risks. Several limitations of the AGI literature base were also\\n    identified, including a limited number of peer reviewed articles and\\n    modelling techniques focused on AGI risk, a lack of specific risk research\\n    in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology.\\n    Recommendations to address the identified issues with AGI risk research\\n    are required to guide AGI design, implementation, and management', 'author(s)': 'Scott McLean, Gemma J. M. Read, Jason Thompson, Chris Baber, Neville A. Stanton, Paul M. Salmon', 'date': 'August 2021', 'keywords': 'AGI, risk assessment, AI safety, Artificial Intelligence', 'page': 1, 'source': 'Journal of Experimental & Theoretical Artificial Intelligence', 'title': 'The risks associated with Artificial General Intelligence: A systematic review', 'type': 'research paper'}, page_content='ARTICLE\\nThe risks associated with Artificial General Intelligence: A \\nsystematic review\\nScott McLean\\na\\n, Gemma J. M. Read\\na\\n, Jason Thompson\\na,b\\n, Chris Baber\\nc\\n, Neville A. Stanton\\na \\nand Paul M. Salmon\\na\\na\\nCentre For Human Factors And Sociotechnical Systems, University Of The Sunshine Coast, Sippy Downs, Australia; \\nb\\nTransport,Health and Urban Design (Thud) Research Lab, Melbourne School of Design, The University of \\nMelbourne, Parkville, Victoria, Australia; \\nc\\nSchool Of Computer Science, University Of Birmingham, Birmingham, UK\\nABSTRACT\\nArtificial General intelligence (AGI) offers enormous benefits for humanity, \\nyet it also poses great risk. The aim of this systematic review was to \\nsummarise the peer reviewed literature on the risks associated with AGI. \\nThe review followed the Preferred Reporting Items for Systematic Reviews \\nand Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed \\neligible for inclusion. Article types included in the review were classified \\nas philosophical discussions, applications of modelling techniques, and \\nassessment of current frameworks and processes in relation to AGI. The \\nreview identified a range of risks associated with AGI, including AGI \\nremoving itself from the control of human owners/managers, being \\ngiven or developing unsafe goals, development of unsafe AGI, AGIs with \\npoor ethics, morals and values; inadequate management of AGI, and \\nexistential risks. Several limitations of the AGI literature base were also'), Document(metadata={'author(s)': 'Daniel Colson', 'date': 'October 18 2024', 'source': 'Time Magazine', 'summary': 'highlights the urgent need for U.S. policymakers to address the rapid \\n    advancements in Artificial General Intelligence (AGI). During a Senate \\n    Judiciary Subcommittee hearing on September 17, 2024, whistleblowers from\\n    leading AI companies, including OpenAI and Google, emphasized that AGI \\n    is no longer a distant speculation but an impending reality. Helen Toner,\\n    a former OpenAI board member, testified that companies like OpenAI,\\n    Google, and Anthropic are earnestly pursuing AGI development. Former\\n    OpenAI researcher William Saunders noted that these companies are \\n    investing billions toward this goal. Senators Josh Hawley and \\n    Richard Blumenthal expressed concern over the lack of regulatory \\n    oversight, warning of potential risks such as cyberattacks, \\n    economic upheaval, and even human extinction. The article \\n    calls for immediate regulatory action, public engagement, \\n    and the implementation of transparency and security mandates \\n    to manage the societal impacts of AGI responsibly.', 'title': 'Silicon Valley Takes Artificial General Intelligence Seriously—Washington Must Too', 'type': 'article'}, page_content='Artificial General Intelligence—machines that can learn and perform any cognitive task that a human can—has long been relegated to the realm of science fiction. But recent developments show that AGI is no longer a distant speculation; it’s an impending reality that demands our immediate attention.On Sept. 17, during a Senate Judiciary Subcommittee hearing titled “Oversight of AI: Insiders’ Perspectives,” whistleblowers from leading AI companies sounded the alarm on the rapid advancement toward AGI and the glaring lack of oversight. Helen Toner, a former board member of OpenAI and director of strategy at Georgetown University’s Center for Security and Emerging Technology, testified that, “The biggest disconnect that I see between AI insider perspectives and public perceptions of AI companies is when it comes to the idea of artificial general intelligence.” She continued that leading AI companies such as OpenAI, Google, and Anthropic are “treating building AGI as an entirely serious'), Document(metadata={'abstract': 'Artificial General intelligence (AGI) offers enormous benefits for humanity,\\n    yet it also poses great risk. The aim of this systematic review was to\\n    summarise the peer reviewed literature on the risks associated with AGI.\\n    The review followed the Preferred Reporting Items for Systematic Reviews\\n    and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed\\n    eligible for inclusion. Article types included in the review were classified\\n    as philosophical discussions, applications of modelling techniques, and\\n    assessment of current frameworks and processes in relation to AGI. The\\n    review identified a range of risks associated with AGI, including AGI\\n    removing itself from the control of human owners/managers, being\\n    given or developing unsafe goals, development of unsafe AGI, AGIs with\\n    poor ethics, morals and values; inadequate management of AGI, and\\n    existential risks. Several limitations of the AGI literature base were also\\n    identified, including a limited number of peer reviewed articles and\\n    modelling techniques focused on AGI risk, a lack of specific risk research\\n    in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology.\\n    Recommendations to address the identified issues with AGI risk research\\n    are required to guide AGI design, implementation, and management', 'author(s)': 'Scott McLean, Gemma J. M. Read, Jason Thompson, Chris Baber, Neville A. Stanton, Paul M. Salmon', 'date': 'August 2021', 'keywords': 'AGI, risk assessment, AI safety, Artificial Intelligence', 'page': 7, 'source': 'Journal of Experimental & Theoretical Artificial Intelligence', 'title': 'The risks associated with Artificial General Intelligence: A systematic review', 'type': 'research paper'}, page_content='Table 1. (Continued).\\nGoertzel and \\nPitt ( 2014 )\\nGeneral \\n(nonspecific)\\nN/A Philosophical \\ndiscussion\\nRisks to humanity Engineer the capability to acquire integrated ethical \\nknowledge; provide rich ethical interaction and \\ninstruction, respecting developmental stages; develop \\nstable, hierarchical goal systems; ensure that the early \\nstages of recursive self-improvement occur relatively \\nslowly and with rich human involvement; tightly link AGI \\nwith the Global Brain; foster deep, consensus-building \\ninteractions between divergent viewpoints; create a \\nmutually supportive community of AGIs; encourage \\nmeasured co-advancement of AGI software and AGI \\nethics theory; develop advanced AGI sooner not later.\\nMiller ( 2019 ) General \\n(nonspecific)\\nN/A Bayesian modelling Dangers of unfriendly AGI Humanity should be more optimistic about its long-term \\nsurvival if we have convincing evidence for believing that \\nboth unfriendly AGI and the great filter are real, than if \\nthere was evidence for thinking that only one of these \\nwould lead to existential risk.\\nNarain et al. \\n( 2019 )\\nManagement of \\nASI\\nManufacturing; communication; and \\ntechnology\\nDiffusion \\nmodelling, \\npredator–prey \\nmodels and \\nhostility models.\\nManagement and control of ASI A gradual diffusion process; \\nThreats pre-empted by long-term social measures; \\nmodelling efforts made to understand the extant \\nperspective of the development of the high-technology \\ndiffusion.\\nNaudé and \\nDimitri \\n( 2020 )\\nGeneral \\n(nonspecific)'), Document(metadata={'abstract': 'Artificial General intelligence (AGI) offers enormous benefits for humanity,\\n    yet it also poses great risk. The aim of this systematic review was to\\n    summarise the peer reviewed literature on the risks associated with AGI.\\n    The review followed the Preferred Reporting Items for Systematic Reviews\\n    and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed\\n    eligible for inclusion. Article types included in the review were classified\\n    as philosophical discussions, applications of modelling techniques, and\\n    assessment of current frameworks and processes in relation to AGI. The\\n    review identified a range of risks associated with AGI, including AGI\\n    removing itself from the control of human owners/managers, being\\n    given or developing unsafe goals, development of unsafe AGI, AGIs with\\n    poor ethics, morals and values; inadequate management of AGI, and\\n    existential risks. Several limitations of the AGI literature base were also\\n    identified, including a limited number of peer reviewed articles and\\n    modelling techniques focused on AGI risk, a lack of specific risk research\\n    in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology.\\n    Recommendations to address the identified issues with AGI risk research\\n    are required to guide AGI design, implementation, and management', 'author(s)': 'Scott McLean, Gemma J. M. Read, Jason Thompson, Chris Baber, Neville A. Stanton, Paul M. Salmon', 'date': 'August 2021', 'keywords': 'AGI, risk assessment, AI safety, Artificial Intelligence', 'page': 6, 'source': 'Journal of Experimental & Theoretical Artificial Intelligence', 'title': 'The risks associated with Artificial General Intelligence: A systematic review', 'type': 'research paper'}, page_content='such as moral reasoning, moral agency; issues related to \\nAI research that may also be further studied by \\nphilosophers.\\nBradley ( 2020 ) Risk \\nmanagement\\nMalicious AI Analysis of current \\nrisk management \\nstandards in \\nrelation to ASI. \\nAI treachery \\nthreat model\\nRisk identification \\nAssessment of likelihood \\nConsequence bias\\nRecognition that current risk management is an incomplete \\napproach to ASIs; risk models to shift from static, \\nanthropomorphic models to focus on data-driven models \\nthat measure intent, manage intent and prevent the \\ntreacherous turn.\\nBrundage \\n( 2014 )\\nMachine ethics N/A Philosophical \\ndiscussion\\nMorals and Ethics No hope of machine ethics building on an existing technical \\ncore; adoption of a broad lens to analyse the inherent \\ndifficulty intelligent action and the complex social \\ncontext in which humans and AGIs will find themselves.\\nChen and Lee \\n( 2019 )\\nPerceptions of \\nthe impact of \\nAGI on \\nhumanity\\nFactory operator; translator; retail \\nsalesperson; tutor; accountant; \\nnews production staff; money \\nmanagement specialist; researcher; \\nartist; surgeon; truck driver; home \\nservice robot; autonomous vehicle; \\nsmart home.\\nQuestionnaire/ \\nsurvey\\nSocial impact of AGI on 12 \\nprofessions, AVs, and smart \\nhomes\\nBetter communication of public perceptions of the benefits \\nand concerns of specific AI applications to AI experts.\\n( Continued )\\n654\\n S. MCLEAN ET AL.'), Document(metadata={'abstract': 'Artificial General intelligence (AGI) offers enormous benefits for humanity,\\n    yet it also poses great risk. The aim of this systematic review was to\\n    summarise the peer reviewed literature on the risks associated with AGI.\\n    The review followed the Preferred Reporting Items for Systematic Reviews\\n    and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed\\n    eligible for inclusion. Article types included in the review were classified\\n    as philosophical discussions, applications of modelling techniques, and\\n    assessment of current frameworks and processes in relation to AGI. The\\n    review identified a range of risks associated with AGI, including AGI\\n    removing itself from the control of human owners/managers, being\\n    given or developing unsafe goals, development of unsafe AGI, AGIs with\\n    poor ethics, morals and values; inadequate management of AGI, and\\n    existential risks. Several limitations of the AGI literature base were also\\n    identified, including a limited number of peer reviewed articles and\\n    modelling techniques focused on AGI risk, a lack of specific risk research\\n    in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology.\\n    Recommendations to address the identified issues with AGI risk research\\n    are required to guide AGI design, implementation, and management', 'author(s)': 'Scott McLean, Gemma J. M. Read, Jason Thompson, Chris Baber, Neville A. Stanton, Paul M. Salmon', 'date': 'August 2021', 'keywords': 'AGI, risk assessment, AI safety, Artificial Intelligence', 'page': 9, 'source': 'Journal of Experimental & Theoretical Artificial Intelligence', 'title': 'The risks associated with Artificial General Intelligence: A systematic review', 'type': 'research paper'}, page_content='Findings from the studies reviewed\\nTypes of analyses\\nA range of analysis methods were identified in the included articles. These included philosophical \\ndiscussions, various modelling approaches, and assessment of current standards and procedures in \\nrelation to AGI risk. One half of the included articles were philosophical discussions on the \\npotential risks associated with AGI systems. These discussions centred upon AGI confinement, \\nAGI control, machine values and morals, risk management, risks to humanity, and the race to \\ndevelop AGI. Whilst philosophical viewpoints on the risks of AGI have provided compelling and \\nostensibly sound arguments, there is considerable disagreement between experts. For example, AI \\nthought leaders Nick Bostrom and Ben Goertzel disagree on containment fails, human attempts to \\nmake AGI goals safe, and AGI not making its own goals safe, among others (Baum et al., 2017 ). \\nWhilst the debate itself is not in the scope of this review, it is important to note that such \\nphilosophical debates have facilitated critical analyses of views on the potential risks associated \\nwith the creation of AGI. Given that little is known about the actual risks of AGI (Baum, 2017 ), \\nphilosophical discussions and thought experiments have been a necessary starting point to \\nprovide direction for AGI risk research.\\nVarious data driven and theoretical models have previously been used to forecast the behaviour'), Document(metadata={'author(s)': 'Daniel Colson', 'date': 'October 18 2024', 'source': 'Time Magazine', 'summary': 'highlights the urgent need for U.S. policymakers to address the rapid \\n    advancements in Artificial General Intelligence (AGI). During a Senate \\n    Judiciary Subcommittee hearing on September 17, 2024, whistleblowers from\\n    leading AI companies, including OpenAI and Google, emphasized that AGI \\n    is no longer a distant speculation but an impending reality. Helen Toner,\\n    a former OpenAI board member, testified that companies like OpenAI,\\n    Google, and Anthropic are earnestly pursuing AGI development. Former\\n    OpenAI researcher William Saunders noted that these companies are \\n    investing billions toward this goal. Senators Josh Hawley and \\n    Richard Blumenthal expressed concern over the lack of regulatory \\n    oversight, warning of potential risks such as cyberattacks, \\n    economic upheaval, and even human extinction. The article \\n    calls for immediate regulatory action, public engagement, \\n    and the implementation of transparency and security mandates \\n    to manage the societal impacts of AGI responsibly.', 'title': 'Silicon Valley Takes Artificial General Intelligence Seriously—Washington Must Too', 'type': 'article'}, page_content='“go slowly and deliberately” in AI development.That’s because the stakes are astronomical. Saunders detailed that AGI could lead to cyberattacks or the creation of “novel biological weapons,” and Toner warned that many leading AI figures believe that in a worst-case scenario AGI “could lead to literal human extinction.”Despite these stakes, the U.S. has instituted almost no regulatory oversight over the companies racing toward AGI. So where does this leave us?First, Washington needs to start taking AGI seriously. The potential risks are too great to ignore. Even in a good scenario, AGI could upend economies and displace millions of jobs, requiring society to adapt. In a bad scenario, AGI could become uncontrollable.Second, we must establish regulatory guardrails for powerful AI systems. Regulation should involve government transparency into what’s going on with the most powerful AI systems that are being created by tech companies. Government transparency will reduce the chances that'), Document(metadata={'abstract': 'Artificial General intelligence (AGI) offers enormous benefits for humanity,\\n    yet it also poses great risk. The aim of this systematic review was to\\n    summarise the peer reviewed literature on the risks associated with AGI.\\n    The review followed the Preferred Reporting Items for Systematic Reviews\\n    and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed\\n    eligible for inclusion. Article types included in the review were classified\\n    as philosophical discussions, applications of modelling techniques, and\\n    assessment of current frameworks and processes in relation to AGI. The\\n    review identified a range of risks associated with AGI, including AGI\\n    removing itself from the control of human owners/managers, being\\n    given or developing unsafe goals, development of unsafe AGI, AGIs with\\n    poor ethics, morals and values; inadequate management of AGI, and\\n    existential risks. Several limitations of the AGI literature base were also\\n    identified, including a limited number of peer reviewed articles and\\n    modelling techniques focused on AGI risk, a lack of specific risk research\\n    in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology.\\n    Recommendations to address the identified issues with AGI risk research\\n    are required to guide AGI design, implementation, and management', 'author(s)': 'Scott McLean, Gemma J. M. Read, Jason Thompson, Chris Baber, Neville A. Stanton, Paul M. Salmon', 'date': 'August 2021', 'keywords': 'AGI, risk assessment, AI safety, Artificial Intelligence', 'page': 12, 'source': 'Journal of Experimental & Theoretical Artificial Intelligence', 'title': 'The risks associated with Artificial General Intelligence: A systematic review', 'type': 'research paper'}, page_content='Lastly, there is no agreed upon single definition of the concept of AGI (Goertzel, 2014 ). Just as the \\nterm AI has many different meanings with the AI community, it appears AGI may suffer the same lack \\nof definitive identity. A finding from the review was the lack of consensus in the terms used within \\nAGI research. An example from the reviewed studies was the reporting of machine intelligence at \\nhuman level and above. Terms included, AGI, artificial superintelligence (ASI), strong AI, High-level- \\nmachine-intelligence (HLHI), superintelligent AI, powerful general artificial intelligence (PAGI), and \\nsuperintelligent agent. In addition, Goertzel ( 2014 ) identified further AGI terms, for example, com -\\nputational intelligence, natural intelligence, cognitive architecture, and biologically inspired cogni -\\ntive architecture. A lack of standardised terminology may be confusing what is already a complex \\nresearch field. Given the potential severe consequences unsafe AGI poses, approaches to make the \\ndiscipline more accessible to researchers from multiple disciplines is required. Future research could \\nbe conducted to standardise a range of AGI terminologies and research fields via consensus of \\nexperts through Delphi studies (Linstone & Turoff, 1975 ).\\nAn important consideration in generalising the findings of the literature is to acknowledge that \\neligible articles may not have been identified in the literature search. However, the search strategy'), Document(metadata={'abstract': 'Artificial General intelligence (AGI) offers enormous benefits for humanity,\\n    yet it also poses great risk. The aim of this systematic review was to\\n    summarise the peer reviewed literature on the risks associated with AGI.\\n    The review followed the Preferred Reporting Items for Systematic Reviews\\n    and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed\\n    eligible for inclusion. Article types included in the review were classified\\n    as philosophical discussions, applications of modelling techniques, and\\n    assessment of current frameworks and processes in relation to AGI. The\\n    review identified a range of risks associated with AGI, including AGI\\n    removing itself from the control of human owners/managers, being\\n    given or developing unsafe goals, development of unsafe AGI, AGIs with\\n    poor ethics, morals and values; inadequate management of AGI, and\\n    existential risks. Several limitations of the AGI literature base were also\\n    identified, including a limited number of peer reviewed articles and\\n    modelling techniques focused on AGI risk, a lack of specific risk research\\n    in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology.\\n    Recommendations to address the identified issues with AGI risk research\\n    are required to guide AGI design, implementation, and management', 'author(s)': 'Scott McLean, Gemma J. M. Read, Jason Thompson, Chris Baber, Neville A. Stanton, Paul M. Salmon', 'date': 'August 2021', 'keywords': 'AGI, risk assessment, AI safety, Artificial Intelligence', 'page': 1, 'source': 'Journal of Experimental & Theoretical Artificial Intelligence', 'title': 'The risks associated with Artificial General Intelligence: A systematic review', 'type': 'research paper'}, page_content='deep learning algorithms to analyse large volumes of data to make predictions regarding behaviour \\nin specific tasks (LeCun et al., 2015 ; Naudé & Dimitri, 2020 ). As such, an ANI’s intelligence is task \\nspecific (or narrow) and cannot transfer to other domains with unknown and uncertain environ -\\nments in which they have not been trained (Firt, 2020 ).\\nIn contrast, an AGI would possess a different level of intelligence (Bostrom, 2014 ), which has \\npreviously been defined as an agent’s ability to achieve goals in a wide range of environments (Legg \\nCONTACT Scott McLean \\n smclean@usc.edu.au \\n University Of The Sunshine Coast, 90 Sippy Downs Drive,Sippy Downs \\n4556, Qld,Australia\\nJOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE \\n2023, VOL. 35, NO. 5, 649–663 \\nhttps://doi.org/10.1080/0952813X.2021.1964003\\n© 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group. \\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial-NoDerivatives License ( http:// \\ncreativecommons.org/licenses/by-nc-nd/4.0/ ), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the \\noriginal work is properly cited, and is not altered, transformed, or built upon in any way.'), Document(metadata={'abstract': 'Artificial General intelligence (AGI) offers enormous benefits for humanity,\\n    yet it also poses great risk. The aim of this systematic review was to\\n    summarise the peer reviewed literature on the risks associated with AGI.\\n    The review followed the Preferred Reporting Items for Systematic Reviews\\n    and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed\\n    eligible for inclusion. Article types included in the review were classified\\n    as philosophical discussions, applications of modelling techniques, and\\n    assessment of current frameworks and processes in relation to AGI. The\\n    review identified a range of risks associated with AGI, including AGI\\n    removing itself from the control of human owners/managers, being\\n    given or developing unsafe goals, development of unsafe AGI, AGIs with\\n    poor ethics, morals and values; inadequate management of AGI, and\\n    existential risks. Several limitations of the AGI literature base were also\\n    identified, including a limited number of peer reviewed articles and\\n    modelling techniques focused on AGI risk, a lack of specific risk research\\n    in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology.\\n    Recommendations to address the identified issues with AGI risk research\\n    are required to guide AGI design, implementation, and management', 'author(s)': 'Scott McLean, Gemma J. M. Read, Jason Thompson, Chris Baber, Neville A. Stanton, Paul M. Salmon', 'date': 'August 2021', 'keywords': 'AGI, risk assessment, AI safety, Artificial Intelligence', 'page': 2, 'source': 'Journal of Experimental & Theoretical Artificial Intelligence', 'title': 'The risks associated with Artificial General Intelligence: A systematic review', 'type': 'research paper'}, page_content='Müller & Bostrom, 2016 ), it is hypothesised that an AGI will have the capability to recursively self- \\nimprove by creating more intelligent versions of itself, as well as altering their pre-programmed \\ngoals (Tegmark, 2017 ). The emergence of AGI could bring about numerous societal challenges, from \\nAGI’s replacing the workforce, manipulation of political and military systems, through to the extinc -\\ntion of humans (Bostrom, 2002 , 2014 ; Salmon et al., 2021 ; Sotala & Yampolskiy, 2015 ). Given the \\nmany known and unknown risks regarding AGI, the scientific community holds concerns regarding \\nthe threats that an AGI may have on humanity (Bradley, 2020 ; Yampolskiy, 2012 ). These concerns \\ninclude malevolent groups creating AGI for malicious use, as well as catastrophic unintended \\nconsequences brought about by apparently well-meaning AGI’s (Salmon et al., 2021 ). There is \\nmuch scepticism among experts as to whether AGI will ever eventuate, and responses to the AGI \\ndebate are broad and range from doing nothing, as an AGI may never be created (Bringsjord et al., \\n2012 ), to the extremes of allowing AGI to destroy humanity and take our place in an evolutionary \\nprocess (Garis, 2005 ).\\nDespite the scepticism, Baum ( 2017 ) identified 45 active AGI research and development projects, \\nincluding Deepmind, Open AI, GoodAI, CommAI, SingularityNET (Baum, 2017 ; Torres, 2019 ). If AGI is'), Document(metadata={'abstract': 'Artificial General intelligence (AGI) offers enormous benefits for humanity,\\n    yet it also poses great risk. The aim of this systematic review was to\\n    summarise the peer reviewed literature on the risks associated with AGI.\\n    The review followed the Preferred Reporting Items for Systematic Reviews\\n    and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed\\n    eligible for inclusion. Article types included in the review were classified\\n    as philosophical discussions, applications of modelling techniques, and\\n    assessment of current frameworks and processes in relation to AGI. The\\n    review identified a range of risks associated with AGI, including AGI\\n    removing itself from the control of human owners/managers, being\\n    given or developing unsafe goals, development of unsafe AGI, AGIs with\\n    poor ethics, morals and values; inadequate management of AGI, and\\n    existential risks. Several limitations of the AGI literature base were also\\n    identified, including a limited number of peer reviewed articles and\\n    modelling techniques focused on AGI risk, a lack of specific risk research\\n    in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology.\\n    Recommendations to address the identified issues with AGI risk research\\n    are required to guide AGI design, implementation, and management', 'author(s)': 'Scott McLean, Gemma J. M. Read, Jason Thompson, Chris Baber, Neville A. Stanton, Paul M. Salmon', 'date': 'August 2021', 'keywords': 'AGI, risk assessment, AI safety, Artificial Intelligence', 'page': 3, 'source': 'Journal of Experimental & Theoretical Artificial Intelligence', 'title': 'The risks associated with Artificial General Intelligence: A systematic review', 'type': 'research paper'}, page_content=\"logical explosion’ AND 'safety' OR 'risk*' OR 'Danger*' OR 'Threat*' Or 'accident*' OR 'security' OR \\n'unintended behaviour' OR 'societal impacts' Or 'Value*' (English: Article). All retrieved articles’ title \\nand abstract were assessed based on the eligibility criteria. Articles that did not align with the \\neligibility criteria were excluded from the review.\\nEligibility criteria\\nA set of inclusion and exclusion criteria were developed to support the identification of studies \\nrelevant to the research question.\\nInclusion criteria\\nArticles were included in the review if they were:\\n● Focused on AI systems with human level intelligence and above (e.g., artificial general intelli -\\ngence and artificial superintelligence);\\n● Focused on risks associated with AGI;\\n● Were published in peer reviewed journals; and\\n● Were published in English.\\nExclusion criteria\\nArticles were excluded from the review where they:\\n● Focused only on artificial intelligence (AI) or artificial narrow intelligence (ANI);\\n● Focused only on AGI algorithms/engineering/architecture/programming;\\n● Focused only on the predicted date of arrival of human level intelligence and above;\\n● Conference proceedings, grey literature, reviews, arXiv articles, journal editorials, books; and\\n● Were commentaries on the different viewpoints related to human level intelligence and above.\\nJOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE\\n 651\"), Document(metadata={'date': 'June 13, 2024', 'source': 'YouTube', 'speaker': 'Roman Yampolskiy', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\", 'speaker_role': 'Guest', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview'}, page_content=\"Roman Yampolskiy: So the definitions we used to have, and people are modifying them a little bit lately, Artificial General Intelligence was a system capable of performing in any domain a human could perform. So kind of you are recreating this average artificial person; they can do cognitive labor, physical labor, where you can get another human to do it. Superintelligence was defined as a system that is superior to all humans in all domains. Now people are starting to refer to AGI as if it's superintelligence. I made a post recently where I argued that for me, at least, if you average out over all the common human tasks, those systems are already smarter than the average human. So under that definition, we have it. ChainLA has this definition of where you're trying to win in all domains. That's what intelligence is. Now, are they smarter than elite individuals in certain domains? Of course not. They're not there yet. But the progress is exponential.\"), Document(metadata={'abstract': 'Artificial General intelligence (AGI) offers enormous benefits for humanity,\\n    yet it also poses great risk. The aim of this systematic review was to\\n    summarise the peer reviewed literature on the risks associated with AGI.\\n    The review followed the Preferred Reporting Items for Systematic Reviews\\n    and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed\\n    eligible for inclusion. Article types included in the review were classified\\n    as philosophical discussions, applications of modelling techniques, and\\n    assessment of current frameworks and processes in relation to AGI. The\\n    review identified a range of risks associated with AGI, including AGI\\n    removing itself from the control of human owners/managers, being\\n    given or developing unsafe goals, development of unsafe AGI, AGIs with\\n    poor ethics, morals and values; inadequate management of AGI, and\\n    existential risks. Several limitations of the AGI literature base were also\\n    identified, including a limited number of peer reviewed articles and\\n    modelling techniques focused on AGI risk, a lack of specific risk research\\n    in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology.\\n    Recommendations to address the identified issues with AGI risk research\\n    are required to guide AGI design, implementation, and management', 'author(s)': 'Scott McLean, Gemma J. M. Read, Jason Thompson, Chris Baber, Neville A. Stanton, Paul M. Salmon', 'date': 'August 2021', 'keywords': 'AGI, risk assessment, AI safety, Artificial Intelligence', 'page': 7, 'source': 'Journal of Experimental & Theoretical Artificial Intelligence', 'title': 'The risks associated with Artificial General Intelligence: A systematic review', 'type': 'research paper'}, page_content='perspective of the development of the high-technology \\ndiffusion.\\nNaudé and \\nDimitri \\n( 2020 )\\nGeneral \\n(nonspecific)\\nN/A Theoretical all-pay \\ncontest model\\nThe race to develop AGI Introducing an intermediate outcome (e.g. second prize \\nrather than one dominant AGI)); \\nusing public procurement of innovation; taxing an AGI; \\naddressing patenting by AI\\nNindler ( 2019 ) Legal N/A Analysis of current \\nlegal and \\ninstitutional \\nframework of the \\nUN\\nLegal capabilities for the \\nmanagement of existential risks \\nposed by AGI\\nAn International Treaty to Regulate AI Research and \\nDevelopment; International Oversight over AI \\nDevelopment; an International Enforcement Agency; the \\nUse of Force as a final possibility f;\\nPueyo ( 2018 ) Environmental/ \\nsocial\\nN/A Philosophical \\ndiscussion\\nEnvironmental and social \\nimplications of superintelligence \\nemerging in an economy \\nshaped by neoliberal policies\\nDegrowth as a viable alternative; systemic change altering \\nthe motivations of economic action; changing values in \\nfirms, governments, and social movements to ease the \\nchange in individual values and reduce the risk of having \\npeople engaged in the development of undesirable \\nforms of AI.\\n( Continued )\\nJOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE\\n 655'), Document(metadata={'abstract': 'Artificial General intelligence (AGI) offers enormous benefits for humanity,\\n    yet it also poses great risk. The aim of this systematic review was to\\n    summarise the peer reviewed literature on the risks associated with AGI.\\n    The review followed the Preferred Reporting Items for Systematic Reviews\\n    and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed\\n    eligible for inclusion. Article types included in the review were classified\\n    as philosophical discussions, applications of modelling techniques, and\\n    assessment of current frameworks and processes in relation to AGI. The\\n    review identified a range of risks associated with AGI, including AGI\\n    removing itself from the control of human owners/managers, being\\n    given or developing unsafe goals, development of unsafe AGI, AGIs with\\n    poor ethics, morals and values; inadequate management of AGI, and\\n    existential risks. Several limitations of the AGI literature base were also\\n    identified, including a limited number of peer reviewed articles and\\n    modelling techniques focused on AGI risk, a lack of specific risk research\\n    in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology.\\n    Recommendations to address the identified issues with AGI risk research\\n    are required to guide AGI design, implementation, and management', 'author(s)': 'Scott McLean, Gemma J. M. Read, Jason Thompson, Chris Baber, Neville A. Stanton, Paul M. Salmon', 'date': 'August 2021', 'keywords': 'AGI, risk assessment, AI safety, Artificial Intelligence', 'page': 14, 'source': 'Journal of Experimental & Theoretical Artificial Intelligence', 'title': 'The risks associated with Artificial General Intelligence: A systematic review', 'type': 'research paper'}, page_content='Garis, H. D. ( 2005 ). The artilect war: Cosmists vs Terrans . ETC Publications.\\nGoertzel, B. ( 2006 ). The hidden pattern . Brown Walker Press.\\nGoertzel, B. ( 2014 ). Artificial general intelligence: Concept, state of the art, and future prospects. Journal of Artificial \\nGeneral Intelligence , 5 (1), 1–48. https://doi.org/10.2478/jagi-2014-0001 \\nGoertzel, B., & Pennachin, C. ( 2007 ). Artificial general intelligence (Vol. 2). Springer.\\nGoertzel, B., & Pitt, J. (2014). Nine ways to bias open-source artificial general intelligence toward friendliness. In Russell \\nBlackford, & Damien Broderick (Eds.), Intelligence unbound (pp. 61–89). Wiley.\\nHolmes, D., Murray, S. J., Perron, A., & Rail, G. ( 2006 ). Deconstructing the evidence-based discourse in health sciences: \\nTruth, power and fascism. International Journal of Evidence-Based Healthcare , 4 (3), 180–186. https://doi.org/10.1111/j. \\n1479-6988.2006.00041.x \\nKaplan, A., & Haenlein, M. ( 2019 ). Siri, Siri, in my hand: Who’s the fairest in the land? On the interpretations, illustrations, \\nand implications of artificial intelligence. Business Horizons , 62 (1), 15–25. https://doi.org/10.1016/j.bushor.2018.08. \\n004 \\nKurzweil, R. ( 2005 ). The singularity is near: When humans transcend biology . Penguin.\\nLeCun, Y., Bengio, Y., & Hinton, G. ( 2015 ). Deep learning. Nature , 521 (7553), 436–444. https://doi.org/10.1038/ \\nnature14539'), Document(metadata={'abstract': 'Artificial General intelligence (AGI) offers enormous benefits for humanity,\\n    yet it also poses great risk. The aim of this systematic review was to\\n    summarise the peer reviewed literature on the risks associated with AGI.\\n    The review followed the Preferred Reporting Items for Systematic Reviews\\n    and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed\\n    eligible for inclusion. Article types included in the review were classified\\n    as philosophical discussions, applications of modelling techniques, and\\n    assessment of current frameworks and processes in relation to AGI. The\\n    review identified a range of risks associated with AGI, including AGI\\n    removing itself from the control of human owners/managers, being\\n    given or developing unsafe goals, development of unsafe AGI, AGIs with\\n    poor ethics, morals and values; inadequate management of AGI, and\\n    existential risks. Several limitations of the AGI literature base were also\\n    identified, including a limited number of peer reviewed articles and\\n    modelling techniques focused on AGI risk, a lack of specific risk research\\n    in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology.\\n    Recommendations to address the identified issues with AGI risk research\\n    are required to guide AGI design, implementation, and management', 'author(s)': 'Scott McLean, Gemma J. M. Read, Jason Thompson, Chris Baber, Neville A. Stanton, Paul M. Salmon', 'date': 'August 2021', 'keywords': 'AGI, risk assessment, AI safety, Artificial Intelligence', 'page': 14, 'source': 'Journal of Experimental & Theoretical Artificial Intelligence', 'title': 'The risks associated with Artificial General Intelligence: A systematic review', 'type': 'research paper'}, page_content='Sotala, K., & Gloor, L. ( 2017 ). Superintelligence as a cause or cure for risks of astronomical suffering. Informatica , 41 (4). \\nhttp://www.informatica.si/index.php/informatica/article/view/1877/1098 \\nSotala, K., & Yampolskiy, R. V. ( 2015 ). Responses to catastrophic AGI risk: A survey. Physica Scripta , 90 (1). 1-33. Article \\n018001. https://doi.org/10.1088/0031-8949/90/1/018001 \\n662\\n S. MCLEAN ET AL.')]\n",
      "\n",
      "Human: Based on the journalistic angle: ethical implications of superintelligent AI and artificial general\n",
      "intelligence (AGI)., provide a draft for an article\n",
      "based on the provided context.\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = prompt_template.format(context=retrieved_docs, question=question)\n",
    "print(\"Final Prompt to LLM:\", formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Draft (for giving advise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Article Structure Suggestions:**\n",
      "\n",
      "1. **Introduction**\n",
      "   - **Information to Include:** A brief overview of AGI and ASI, highlighting the rapid advancements in AI technology and the lack of regulatory measures. Introduce the stakes involved, including existential threats and impacts on human well-being.\n",
      "   - **Source:** “go slowly and deliberately” in AI development. \"The risks associated with Artificial General Intelligence: A systematic review\" by Scott McLean et al., 2021.\n",
      "\n",
      "2. **Understanding AGI and ASI**\n",
      "   - **Information to Include:** Define AGI and ASI, explaining how they differ from current AI systems. Discuss the implications of achieving AGI, including the potential for self-improvement and the risks involved.\n",
      "   - **Source:** \"Artificial General Intelligence—machines that can learn and perform any cognitive task that a human can,\" \"The risks associated with Artificial General Intelligence: A systematic review\" by Scott McLean et al., 2021.\n",
      "\n",
      "3. **Current State of Regulation**\n",
      "   - **Information to Include:** Analyze the existing regulatory landscape in the U.S. regarding AGI and ASI. Include testimony from experts and whistleblowers regarding the urgent need for oversight.\n",
      "   - **Source:** \"Senator Richard Blumenthal (D-CT), the subcommittee Chair, was even more direct... don’t trust Big Tech\" from the Senate Judiciary Subcommittee hearing, 2023.\n",
      "\n",
      "4. **Risks Associated with AGI**\n",
      "   - **Information to Include:** Discuss the various risks identified in the literature, including existential risks, potential for cyberattacks, and development of \"novel biological weapons.\"\n",
      "   - **Source:** \"The review identified a range of risks associated with AGI, including AGI removing itself from the control of human owners/managers\" from \"The risks associated with Artificial General Intelligence: A systematic review\" by Scott McLean et al., 2021.\n",
      "\n",
      "5. **Human and Societal Impact**\n",
      "   - **Information to Include:** Explore the broader implications of AGI on human life, economy, and societal structures. Discuss potential suffering and existential threats, linking these to concepts like ikigai.\n",
      "   - **Source:** \"Even in a good scenario, AGI could upend economies and displace millions of jobs\" from \"go slowly and deliberately\" in AI development.\n",
      "\n",
      "6. **Call to Action**\n",
      "   - **Information to Include:** Emphasize the need for immediate action from policymakers to institute regulatory frameworks. Highlight public engagement as essential for addressing AGI risks.\n",
      "   - **Source:** \"It’s time for policymakers to begin to get their heads out of the cloud\" from \"go slowly and deliberately\" in AI development.\n",
      "\n",
      "7. **Conclusion**\n",
      "   - **Information to Include:** Recap the urgency of addressing AGI and ASI risks, stressing the importance of proactive regulation and societal preparation.\n",
      "   - **Source:** \"Ignoring the potentially imminent challenges of AGI won’t make them disappear\" from \"go slowly and deliberately\" in AI development.\n",
      "\n",
      "**Suggestions for Further Investigation:**\n",
      "1. What specific regulatory measures can be implemented to ensure the safe development of AGI?\n",
      "2. How do different countries approach the regulation of AI, and what can the U.S. learn from them?\n",
      "3. What are the perspectives of leading AI developers on the risks associated with AGI?\n",
      "4. How can the public be effectively engaged in discussions about AGI and its implications?\n",
      "5. What ethical frameworks are being proposed to guide AGI development?\n",
      "\n",
      "By structuring the article in this way, the journalist can provide a comprehensive view of the urgent need for action regarding AGI and ASI, while also engaging readers with critical questions that require further exploration.\n"
     ]
    }
   ],
   "source": [
    "# We choose angle #1 about \n",
    "\n",
    "#**The Ethical Implications of Superintelligent AI**:\n",
    "\n",
    "# Define the retriever to get the documents from the vectorstore\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type = \"mmr\",  # Use Maximal Marginal Relevance (MMR) for search\n",
    "    search_kwargs={\"k\": 30,\n",
    "                   \"lambda_mult\": 0.7,\n",
    "                   \"fetch_k\": 50})\n",
    "\n",
    "system_prompt = \"\"\"You are an AI assistant to a journalist.\n",
    "Your goal is to generate accurate, concise answers using only the provided context.\n",
    "Do not make assumptions or add information that is not explicitly present in the context.\n",
    "\n",
    "When referencing or using information, quote the source directly using the information in the metadata (\"source\", \"type\", \"title\", \"date\").\n",
    "Use quotation marks \"QUOTE\" when possible. Only use exact phrases from the context when quoting to ensure accuracy.\n",
    "\n",
    "Here is the context: {context}\n",
    "\"\"\"\n",
    "human_prompt = \"{question}\"\n",
    "\n",
    "#Define the prompt which will be used by the RAG\n",
    "#This takes the prompts as strings, and converts them into the format the model expects.\n",
    "def create_prompt(system_prompt, human_prompt):\n",
    "    system_message = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "    human_message = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "    return ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "\n",
    "\n",
    "\n",
    "angle = \"\"\"The world is not prepared for Artificial Super Intelligence (ASI)\n",
    "or Artificial General Intelligence (AGI), and policy is not moving to control and monitor\n",
    "the businesses who lead the field. The threats of AGI are not only existential, but include suffering and ikigai as well.\"\"\"\n",
    "\n",
    "question = f\"\"\"Based on the journalistic angle: {angle}, provide suggestions for how \n",
    "the journalist may structure his article. This should include: \n",
    "1. What order the information should be presented in, and what sections the article could include.\n",
    "2. What information should be included in each section (with sources: type, title, date - this IS found in the metadata).\n",
    "3. Suggestions for questions that haven't been answered, that the journalist might go investigate.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = create_prompt(system_prompt, human_prompt)\n",
    "\n",
    "#Define a function that takes all the documents fetched by the receiver and formats \n",
    "#them into a single string, with double spaces between each document.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Set up the RAG chain\n",
    "#1. First the receiver takes the question and fetches the relevant dokuments from the vector store\n",
    "#2. The question is passed through to the next step of the chain\n",
    "#3. The question and context (documents) are passed to the prompt template\n",
    "#4. Prompt template is sent to the LLM\n",
    "#5. The output from the LLM is parsed into a string\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm  # ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save response as a word file\n",
    "with open(\"article.docx\", \"w\") as f:\n",
    "    f.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Draft: more structured approach to giving advise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed for section: Here are five suggested article sections along with their relevant sources:\n",
      "Response validation failed!\n",
      "Here are five suggested article sections along with their relevant sources:\n",
      "\n",
      "1. **Introduction: The Imminent Threat of AGI and ASI**\n",
      "   - Source: \"Silicon Valley Takes Artificial General Intelligence Seriously—Washington Must Too,\" Time Magazine\n",
      "\n",
      "2. **Current State of Policy and Regulation**\n",
      "   - Source: \"Silicon Valley Takes Artificial General Intelligence Seriously—Washington Must Too,\" Time Magazine\n",
      "\n",
      "3. **The Risks Associated with AGI**\n",
      "   - Source: \"The risks associated with Artificial General Intelligence: A systematic review,\" Journal of Experimental & Theoretical Artificial Intelligence\n",
      "\n",
      "4. **Potential Catastrophic Outcomes of AGI**\n",
      "   - Source: \"Silicon Valley Takes Artificial General Intelligence Seriously—Washington Must Too,\" Time Magazine\n",
      "   - Source: \"The risks associated with Artificial General Intelligence: A systematic review,\" Journal of Experimental & Theoretical Artificial Intelligence\n",
      "\n",
      "5. **Call to Action: The Need for Robust Oversight and Public Engagement**\n",
      "   - Source: \"Silicon Valley Takes Artificial General Intelligence Seriously—Washington Must Too,\" Time Magazine\n",
      "\n",
      "Sections that do not have corresponding sources explicitly mentioned do not apply in this case, as all suggested sections have relevant sources.\n"
     ]
    }
   ],
   "source": [
    "# We choose angle #1 about \n",
    "\n",
    "#**The Ethical Implications of Superintelligent AI**:\n",
    "\n",
    "# Define the retriever to get the documents from the vectorstore\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type = \"mmr\",  # Use Maximal Marginal Relevance (MMR) for search\n",
    "    search_kwargs={\"k\": 25,\n",
    "                   \"lambda_mult\": 0.7,\n",
    "                   \"fetch_k\": 50})\n",
    "\n",
    "system_prompt = \"\"\"You are an AI assistant to a journalist.\n",
    "Your goal is to generate accurate, concise answers using only the provided context.\n",
    "Do not make assumptions or add information that is not explicitly present in the context.\n",
    "\n",
    "When referencing or using information, quote the source directly using the metadata fields:\n",
    "- Title: Refer to the title of the source.\n",
    "- Source: Mention the publication or source name.\n",
    "- Date: Include the publication date.\n",
    "\n",
    "Use quotation marks \"QUOTE\" for any phrases or sentences quoted directly from the context. \n",
    "If you are unsure about a piece of information, explicitly state that it is not mentioned in the context.\n",
    "\n",
    "Here is the context: {context}\n",
    "\"\"\"\n",
    "human_prompt = \"{question}\"\n",
    "\n",
    "#Define the prompt which will be used by the RAG\n",
    "#This takes the prompts as strings, and converts them into the format the model expects.\n",
    "def create_prompt(system_prompt, human_prompt):\n",
    "    system_message = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "    human_message = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "    return ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "\n",
    "\n",
    "\n",
    "angle = \"\"\"The world is not prepared for Artificial Super Intelligence (ASI)\n",
    "or Artificial General Intelligence (AGI), and policy is not moving to control and monitor\n",
    "the businesses leading the field. Additionally, what are the threats of AGI?\"\"\"\n",
    "\n",
    "question = f\"\"\"Based on the journalistic angle: {angle}, suggest 5 article sections and associate\n",
    "relevant sources (from the metadata fields \"title\" and \"source\") for each section.\n",
    "If some sections do not have corresponding sources, explicitly mention this.\"\"\"\n",
    "\n",
    "prompt_template = create_prompt(system_prompt, human_prompt)\n",
    "\n",
    "#Define a function that takes all the documents fetched by the receiver and formats \n",
    "#them into a single string, with double spaces between each document.\n",
    "\"\"\" def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs) \"\"\"\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        metadata = doc.metadata\n",
    "        formatted_docs.append(\n",
    "            f\"Title: {metadata.get('title', 'Title not specified')}\\n\"\n",
    "            f\"Source: {metadata.get('source', 'Source not specified')}\\n\"\n",
    "            f\"Date: {metadata.get('date', 'Date not specified')}\\n\"\n",
    "            f\"Content: {doc.page_content}\"\n",
    "            \"---------------\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Set up the RAG chain\n",
    "#1. First the receiver takes the question and fetches the relevant dokuments from the vector store\n",
    "#2. The question is passed through to the next step of the chain\n",
    "#3. The question and context (documents) are passed to the prompt template\n",
    "#4. Prompt template is sent to the LLM\n",
    "#5. The output from the LLM is parsed into a string\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableMap({\n",
    "        \"context\": retriever | format_docs, \n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | prompt_template\n",
    "    | llm  # ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def validate_response(response):\n",
    "    for section in response.split(\"\\n\\n\"):\n",
    "        if \"Title:\" not in section or \"Source:\" not in section:\n",
    "            print(f\"Validation failed for section: {section}\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "response = rag_chain.invoke(question)\n",
    "if not validate_response(response):\n",
    "    print(\"Response validation failed!\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save response as a word file\n",
    "with open(\"article.docx\", \"w\") as f:\n",
    "    f.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full prompt successfully saved to full_prompt_with_context.txt\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the chunks for the given question\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "# Define the file path where you want to save the output\n",
    "file_path = \"full_prompt_with_context.txt\"\n",
    "\n",
    "# Compose the context as the LLM would receive it\n",
    "formatted_docs = format_docs(retrieved_docs)  # Use the same format_docs function for consistency\n",
    "\n",
    "# Compose the full prompt\n",
    "full_prompt = f\"\"\"\n",
    "System Message:\n",
    "{system_prompt}\n",
    "\n",
    "Human Message:\n",
    "{question}\n",
    "\n",
    "Context Provided to the LLM:\n",
    "{formatted_docs}\n",
    "\"\"\"\n",
    "\n",
    "# Write everything to the file\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"Full Prompt Sent to the LLM:\\n\\n\")\n",
    "    file.write(full_prompt)\n",
    "\n",
    "print(f\"Full prompt successfully saved to {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
