{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages and modules\n",
    "#Get the required modules/packages\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "import re\n",
    "from langchain.schema import Document \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import TextSplitter\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Access the API key from the environment variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "#change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "# Load the vectorstore\n",
    "\n",
    "persist_directory = \"chroma_storage\"\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(\"Vector store loaded from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete vectorstore (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: 'chroma_storage\\\\chroma.sqlite3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchroma_storage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Delete the directory and its contents\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:739\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;66;03m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:617\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    615\u001b[0m             os\u001b[38;5;241m.\u001b[39munlink(fullname)\n\u001b[0;32m    616\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 617\u001b[0m             \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m     os\u001b[38;5;241m.\u001b[39mrmdir(path)\n",
      "File \u001b[1;32mc:\\Users\\cleme\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:615\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 615\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    617\u001b[0m         onerror(os\u001b[38;5;241m.\u001b[39munlink, fullname, sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: 'chroma_storage\\\\chroma.sqlite3'"
     ]
    }
   ],
   "source": [
    "# Delete the existing vectorstore\n",
    "import shutil\n",
    "\n",
    "# Specify the directory you used for persistent storage\n",
    "persist_directory = \"chroma_storage\"\n",
    "\n",
    "# Delete the directory and its contents\n",
    "shutil.rmtree(persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the interview transcript is prepared to be embedded in the vector store\n",
    "\n",
    "1. The full text is grabbed from the document\n",
    "2. Metadata such as speaker name, role, and bio are defined. But also metadata\n",
    "    such as type, source, data, etc.\n",
    "3. LangChainDocument is prepared\n",
    "4. Splitter is defined (chunk size + overlap)\n",
    "5. for loop splits the document into chunks and attaches relevant metadata\n",
    "6. The chunks are appended in an empty list named \"splits\"\n",
    "7. The chunks (or splits) are embedded into the vectorstore Chroma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interview without metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Lex Fridman: What to you is the probability that superintelligent AI will destroy all human civilization?\n",
      "\n",
      "Roman Yampolskiy: What's the timeframe?\n",
      "\n",
      "Lex Fridman: Let's say a hundred years, in the next  ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: So the problem of controlling AGI or superintelligence, in my opinion, is like the problem of creating a perpetual safety machine by analogy with a perpetual motion machine. It's imp ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Roman Yampolskiy', 'speaker_role': 'Guest', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\"}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: So there is an incremental improvement of systems leading up to AGI. To you, it doesn't matter if we can keep those safe. There's going to be one level of system at which you cannot possi ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: Exactly. But the systems we have today have the capability of causing X amount of damage. So then they fail. That's all we get. If we develop systems capable of impacting all of huma ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: So there is like an unlimited level of creativity in terms of how humans could be killed. But, you know, we could still investigate possible ways of doing it, not how to do it. But at the ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: They are limited by how imaginative we are. If you are that much smarter, that much more creative, if you are capable of thinking across multiple domains, doing novel research in phy ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Roman Yampolskiy', 'speaker_role': 'Guest', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\"}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: So are you thinking about mass murder and destruction of human civilization? Are you thinking of... with squirrels, you put them in a zoo and they don't really know they're in a zoo. If w ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: I think about a lot of things. So there is existential risk, where everyone's dead. There is suffering risk, where everyone wishes they were dead. We also have the idea of Ikigai ris ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: So the Japanese concept of Ikigai is finding something that allows you to make money, you are good at it, and society says we need it. So, like, you have this awesome job; you are a  ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Roman Yampolskiy', 'speaker_role': 'Guest', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\"}\n",
      "--------------------------------------------------\n",
      "Content: modified in one generation. It's not a slow process where we get to figure out how to live that new lifestyle, but it's pretty quick. ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Unknown', 'speaker_role': 'Unknown', 'speaker_bio': 'No bio available'}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: In that world, can't humans do what humans currently do with chess? Play each other, have tournaments? Even though AI systems are far superior this time in chess. So we just create artifi ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: Okay, so why is that not a likely outcome? Why can't AI systems create video games for us to lose ourselves in, each with an individual video game universe?\n",
      "\n",
      "Roman Yampolskiy: Some people ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: Personal universes. Personal universes.\n",
      "\n",
      "Lex Fridman: So that's one of the possible outcomes. But what in general is the idea of the paper? It's looking at multiple agents that are h ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: In order to solve the value alignment problem, I'm trying to formalize it a little better. Usually, we're talking about getting AIs to do what we want, which is not well defined. Are ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Roman Yampolskiy', 'speaker_role': 'Guest', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\"}\n",
      "--------------------------------------------------\n",
      "Content: It's going to hit a point where you can't tell the difference. And if you can't tell if it's real or not, what's the difference? ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Unknown', 'speaker_role': 'Unknown', 'speaker_bio': 'No bio available'}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: So basically, give up on value alignment, create an entire... it's like the multiverse theory. Just create an entire universe for you. What your values, you still have to align with that  ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: It seems contradictory. I haven't seen anyone explain what it means outside of words, which pack a lot. Make it good, make it desirable, make it something they don't regret. But how  ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: Right, but the examples you gave, some of them are, for example, two different religions saying this is our holy site and we are not willing to compromise it in any way. If you can m ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: If we go back to that idea of simulation and this is entertainment giving meaning to us, the question is how much suffering is reasonable for a video game. So yeah, I don't mind a vi ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: So we know there are some humans who, because of a mutation, don't experience physical pain. So at least physical pain can be mutated out, reengineered out. Suffering in terms of mea ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: Okay, so what's the S risk? What are the possible things that you're imagining with S risk? So mass suffering of humans, what are we talking about there? Caused by AGI?\n",
      "\n",
      "Roman Yampolskiy: ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: So there are different malevolent agents; some may just gain personal benefit and sacrifice others to that cause. Others we know for effect try to kill as many people as possible. Wh ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: Again, I would like to assume that normal people never think like that. It's always some sort of psychopaths.\n",
      "\n",
      "Lex Fridman: But yeah, and to you, AGI systems can carry that and be mo ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: So to you, eventually, this is—we're heading off a cliff.\n",
      "\n",
      "Roman Yampolskiy: If we create general superintelligences, I don't see a good long-term outcome for humanity. The only way to wi ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: So the definitions we used to have, and people are modifying them a little bit lately, Artificial General Intelligence was a system capable of performing in any domain a human could  ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Roman Yampolskiy', 'speaker_role': 'Guest', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\"}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: See, I'm much more concerned about social engineering. So to me, AI's ability to do something in the physical world is like the lowest hanging fruit. The easiest set of methods is by just ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: Human level is general. In the domain of expertise of humans, we know how to do human things. I don't speak dog language. I should be able to pick it up. If I'm a general intelligenc ...\n",
      "Metadata: {'source': 'interview', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\" from docx import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document as LangChainDocument\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load the Word document and extract text\n",
    "doc = Document(\"corrected_interview.docx\")\n",
    "interview_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Define speaker metadata\n",
    "speaker_info = {\n",
    "    \"Lex Fridman\": {\n",
    "        \"role\": \"Interviewer\",\n",
    "        \"bio\": \"Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.\"\n",
    "    },\n",
    "    \"Roman Yampolskiy\": {\n",
    "        \"role\": \"Guest\",\n",
    "        \"bio\": \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Identify speaker based on keywords\n",
    "def identify_speaker(text):\n",
    "    if \"Lex Fridman\" in text:\n",
    "        return \"Lex Fridman\"\n",
    "    elif \"Roman Yampolskiy\" in text:\n",
    "        return \"Roman Yampolskiy\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Create the initial LangChain document with the entire text and minimal metadata\n",
    "documents = [LangChainDocument(page_content=interview_text, metadata={\"source\": \"interview\"})]\n",
    "\n",
    "# Initialize RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# Split the document into chunks with metadata\n",
    "splits = []\n",
    "for chunk in splitter.split_documents(documents):\n",
    "    speaker = identify_speaker(chunk.page_content)\n",
    "    \n",
    "    # Add detailed metadata for each chunk\n",
    "    chunk.metadata.update({\n",
    "        \"speaker\": speaker,\n",
    "        \"speaker_role\": speaker_info.get(speaker, {}).get(\"role\", \"Unknown\"),\n",
    "        \"speaker_bio\": speaker_info.get(speaker, {}).get(\"bio\", \"No bio available\")\n",
    "    })\n",
    "    splits.append(chunk)\n",
    "\n",
    "# Verify the splits and metadata\n",
    "for doc in splits:\n",
    "    print(\"Content:\", doc.page_content[:200], \"...\")  # Print first 200 characters for brevity\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Embed the chunks into Chroma vectorstore\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings(),\n",
    "                                    persist_directory = persist_directory)\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for interview embedding interview with updated metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Lex Fridman: What to you is the probability that superintelligent AI will destroy all human civilization?\n",
      "\n",
      "Roman Yampolskiy: What's the timeframe?\n",
      "\n",
      "Lex Fridman: Let's say a hundred years, in the next  ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: So the problem of controlling AGI or superintelligence, in my opinion, is like the problem of creating a perpetual safety machine by analogy with a perpetual motion machine. It's imp ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Roman Yampolskiy', 'speaker_role': 'Guest', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\"}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: So there is an incremental improvement of systems leading up to AGI. To you, it doesn't matter if we can keep those safe. There's going to be one level of system at which you cannot possi ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: Exactly. But the systems we have today have the capability of causing X amount of damage. So then they fail. That's all we get. If we develop systems capable of impacting all of huma ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: So there is like an unlimited level of creativity in terms of how humans could be killed. But, you know, we could still investigate possible ways of doing it, not how to do it. But at the ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: They are limited by how imaginative we are. If you are that much smarter, that much more creative, if you are capable of thinking across multiple domains, doing novel research in phy ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Roman Yampolskiy', 'speaker_role': 'Guest', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\"}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: So are you thinking about mass murder and destruction of human civilization? Are you thinking of... with squirrels, you put them in a zoo and they don't really know they're in a zoo. If w ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: I think about a lot of things. So there is existential risk, where everyone's dead. There is suffering risk, where everyone wishes they were dead. We also have the idea of Ikigai ris ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: So the Japanese concept of Ikigai is finding something that allows you to make money, you are good at it, and society says we need it. So, like, you have this awesome job; you are a  ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Roman Yampolskiy', 'speaker_role': 'Guest', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\"}\n",
      "--------------------------------------------------\n",
      "Content: modified in one generation. It's not a slow process where we get to figure out how to live that new lifestyle, but it's pretty quick. ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Unknown', 'speaker_role': 'Unknown', 'speaker_bio': 'No bio available'}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: In that world, can't humans do what humans currently do with chess? Play each other, have tournaments? Even though AI systems are far superior this time in chess. So we just create artifi ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: Okay, so why is that not a likely outcome? Why can't AI systems create video games for us to lose ourselves in, each with an individual video game universe?\n",
      "\n",
      "Roman Yampolskiy: Some people ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: Personal universes. Personal universes.\n",
      "\n",
      "Lex Fridman: So that's one of the possible outcomes. But what in general is the idea of the paper? It's looking at multiple agents that are h ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: In order to solve the value alignment problem, I'm trying to formalize it a little better. Usually, we're talking about getting AIs to do what we want, which is not well defined. Are ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Roman Yampolskiy', 'speaker_role': 'Guest', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\"}\n",
      "--------------------------------------------------\n",
      "Content: It's going to hit a point where you can't tell the difference. And if you can't tell if it's real or not, what's the difference? ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Unknown', 'speaker_role': 'Unknown', 'speaker_bio': 'No bio available'}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: So basically, give up on value alignment, create an entire... it's like the multiverse theory. Just create an entire universe for you. What your values, you still have to align with that  ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: It seems contradictory. I haven't seen anyone explain what it means outside of words, which pack a lot. Make it good, make it desirable, make it something they don't regret. But how  ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: Right, but the examples you gave, some of them are, for example, two different religions saying this is our holy site and we are not willing to compromise it in any way. If you can m ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: If we go back to that idea of simulation and this is entertainment giving meaning to us, the question is how much suffering is reasonable for a video game. So yeah, I don't mind a vi ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: So we know there are some humans who, because of a mutation, don't experience physical pain. So at least physical pain can be mutated out, reengineered out. Suffering in terms of mea ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: Okay, so what's the S risk? What are the possible things that you're imagining with S risk? So mass suffering of humans, what are we talking about there? Caused by AGI?\n",
      "\n",
      "Roman Yampolskiy: ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: So there are different malevolent agents; some may just gain personal benefit and sacrifice others to that cause. Others we know for effect try to kill as many people as possible. Wh ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: Again, I would like to assume that normal people never think like that. It's always some sort of psychopaths.\n",
      "\n",
      "Lex Fridman: But yeah, and to you, AGI systems can carry that and be mo ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: So to you, eventually, this is—we're heading off a cliff.\n",
      "\n",
      "Roman Yampolskiy: If we create general superintelligences, I don't see a good long-term outcome for humanity. The only way to wi ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: So the definitions we used to have, and people are modifying them a little bit lately, Artificial General Intelligence was a system capable of performing in any domain a human could  ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Roman Yampolskiy', 'speaker_role': 'Guest', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\"}\n",
      "--------------------------------------------------\n",
      "Content: Lex Fridman: See, I'm much more concerned about social engineering. So to me, AI's ability to do something in the physical world is like the lowest hanging fruit. The easiest set of methods is by just ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n",
      "Content: Roman Yampolskiy: Human level is general. In the domain of expertise of humans, we know how to do human things. I don't speak dog language. I should be able to pick it up. If I'm a general intelligenc ...\n",
      "Metadata: {'source': 'YouTube', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview', 'date': 'June 13, 2024', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'speaker': 'Lex Fridman', 'speaker_role': 'Interviewer', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from docx import Document as DocxDocument\n",
    "from langchain.schema import Document as LangChainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load the Word document and extract text\n",
    "doc = DocxDocument(\"corrected_interview.docx\")\n",
    "interview_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Define speaker metadata\n",
    "speaker_info = {\n",
    "    \"Lex Fridman\": {\n",
    "        \"role\": \"Interviewer\",\n",
    "        \"bio\": \"Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.\"\n",
    "    },\n",
    "    \"Roman Yampolskiy\": {\n",
    "        \"role\": \"Guest\",\n",
    "        \"bio\": \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to identify speaker based on keywords in the text\n",
    "def identify_speaker(text):\n",
    "    if \"Lex Fridman\" in text:\n",
    "        return \"Lex Fridman\"\n",
    "    elif \"Roman Yampolskiy\" in text:\n",
    "        return \"Roman Yampolskiy\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Initialize Document with minimal metadata\n",
    "documents = [LangChainDocument(page_content=interview_text, metadata={\"source\": \"YouTube\"})]\n",
    "\n",
    "# Initialize RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# Split the document into chunks and add metadata\n",
    "splits = []\n",
    "for chunk in splitter.split_documents(documents):\n",
    "    # Identify the speaker for each chunk\n",
    "    speaker = identify_speaker(chunk.page_content)\n",
    "    \n",
    "    # Add detailed metadata for each chunk\n",
    "    chunk.metadata.update({\n",
    "        \"source\": \"YouTube\",\n",
    "        \"title\": \"P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman\",\n",
    "        \"type\": \"interview\",\n",
    "        \"date\": \"June 13, 2024\",\n",
    "        \"summary\": (\n",
    "            \"\"\"In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\n",
    "            about the safety of superintelligent AI, likening its control to the impossible creation\n",
    "            of a perpetual motion machine, emphasizing the existential risks that come from potential\n",
    "            mistakes in AI development. He discusses various risks associated with advanced AI, including\n",
    "            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\n",
    "            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\n",
    "            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\n",
    "            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\n",
    "            a point where we cannot defend against its potential threats, leading him to conclude that the\n",
    "            best course of action might be to avoid creating superintelligent systems altogether.\"\"\"\n",
    "        ),\n",
    "        \"speaker\": speaker,\n",
    "        \"speaker_role\": speaker_info.get(speaker, {}).get(\"role\", \"Unknown\"),\n",
    "        \"speaker_bio\": speaker_info.get(speaker, {}).get(\"bio\", \"No bio available\")\n",
    "    })\n",
    "    splits.append(chunk)\n",
    "\n",
    "# Verify the splits and metadata\n",
    "for doc in splits:\n",
    "    print(\"Content:\", doc.page_content[:200], \"...\")\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Embed the chunks into Chroma vectorstore\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings(),\n",
    "                                    persist_directory=persist_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -qU pypdf\n",
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the PDF document\n",
    "loader = PyPDFLoader(\"AGI Research article.pdf\")\n",
    "\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'AGI Research article.pdf', 'page': 0}\n",
      "\n",
      "& Hutter, 2006 ), and the ability to achieve complex goals in complex environments (Goertzel, 2006 ). \n",
      "Whilst current ANI systems have typically been used as tools to support human behaviours, an AGI \n",
      "system would be an autonomous agent that can learn in an unsupervised manner (Firt, 2020 ; Torres, \n",
      "2019 ). Whilst, AGI does not currently exist, it is expected to arrive sometime this century (Müller & \n",
      "Bostrom, 2016 ).\n",
      "Although ANI systems such as Uber’s automated vehicles can create safety risks (Stanton et al., \n",
      "2019 ), they do not, at present, pose a significant threat to humanity (Bentley, 2018 ). This is not the \n",
      "case with AGI, with many scholars discussing potential existential threats (Salmon et al., 2021 ). The \n",
      "risks associated with AGI are generated by the challenge of controlling an agent that is substantially \n",
      "more intelligent than us (Baum, 2017 ). The exponential rate at which technology is advancing, such \n",
      "as in the areas of computing power, data science, neuroscience, and bioengineering, has led many \n",
      "scholars to believe that an intelligence explosion will be reached in the near future (Kurzweil, 2005 ; \n",
      "Naudé & Dimitri, 2020 ). An intelligence explosion would see AI exceed human-level intelligence \n",
      "(Chalmers, 2009 ). At this point, which is estimated to occur between 2040 to 2070 (Baum et al., 2011 ; \n",
      "Müller & Bostrom, 2016 ), it is hypothesised that an AGI will have the capability to recursively self- \n",
      "improve by creating more intelligent versions of itself, as well as altering their pre-programmed \n",
      "goals (Tegmark, 2017 ). The emergence of AGI could bring about numerous societal challenges, from \n",
      "AGI’s replacing the workforce, manipulation of political and military systems, through to the extinc -\n",
      "tion of humans (Bostrom, 2002 , 2014 ; Salmon et al., 2021 ; Sotala & Yampolskiy, 2015 ). Given the \n",
      "many known and unknown risks regarding AGI, the scientific community holds concerns regarding \n",
      "the threats that an AGI may have on humanity (Bradley, 2020 ; Yampolskiy, 2012 ). These concerns \n",
      "include malevolent groups creating AGI for malicious use, as well as catastrophic unintended \n",
      "consequences brought about by apparently well-meaning AGI’s (Salmon et al., 2021 ). There is \n",
      "much scepticism among experts as to whether AGI will ever eventuate, and responses to the AGI \n",
      "debate are broad and range from doing nothing, as an AGI may never be created (Bringsjord et al., \n",
      "2012 ), to the extremes of allowing AGI to destroy humanity and take our place in an evolutionary \n",
      "process (Garis, 2005 ).\n",
      "Despite the scepticism, Baum ( 2017 ) identified 45 active AGI research and development projects, \n",
      "including Deepmind, Open AI, GoodAI, CommAI, SingularityNET (Baum, 2017 ; Torres, 2019 ). If AGI is \n",
      "successfully developed, it is argued that there will be only one chance to ensure that the design, \n",
      "implementation and operation of AGI is appropriately managed, as rapid advances will immediately \n",
      "render the initial AGI obsolete (Bostrom, 2014 ). This is highly problematic when considering risk \n",
      "management, as the initial risk controls may also be ineffective as the AGI redesigns and self- \n",
      "improves. As such, there is an urgent need to understand and develop appropriate risk controls \n",
      "now, to ensure the creation of safe AGI’s and continued and effective management of associated \n",
      "risks as they develop (Salmon et al., 2021 ; Sotala & Yampolskiy, 2015 ). Salmon et al. ( 2021 ), for \n",
      "example, recently outlined a research agenda designed to ensure that appropriate design and risk \n",
      "management methods are immediately embedded in AGI design. Despite calls such as this, the \n",
      "extent to which the research community is actively exploring the risks associated with AGI in \n",
      "scientific research is not clear (Baum, 2017 ). Moreover, the specific nature of the risks associated \n",
      "with AGI is not often made clear, with discussions focusing more on general existential threats such \n",
      "as AGI systems deciding that humans are no longer required (Bostrom, 2014 ). The literature on the \n",
      "risks associated with AGI has grown substantially within the last decade, and includes numerous \n",
      "books, government and academic white papers, website blogs and articles, and conference proceed -\n",
      "ings, among others (Sotala & Yampolskiy, 2015 ). However, the extent to which this has translated \n",
      "into formal scientific studies exploring the risks associated with AGI is not clear. The purpose of this \n",
      "systematic review is therefore to examine and report on the peer reviewed scientific literature that \n",
      "has specifically investigated the risks associated with AGI. The intention was to determine the level of \n",
      "scientific inquiry in this area and to identify specifically what forms of risk are being explored. As such \n",
      "the specific research question investigated for the current systematic review was: What are the risks \n",
      "associated with Artificial General Intelligence ?\n",
      "650\n",
      " S. MCLEAN ET AL.\n"
     ]
    }
   ],
   "source": [
    "# Check content of pages\n",
    "print(f\"{pages[0].metadata}\\n\")\n",
    "print(pages[2].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully embedded the chunks in Chroma Vector Store.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document as LangChainDocument\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to handle async in Jupyter or similar environments\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Step 1: Load the PDF Document\n",
    "file_path = \"AGI Research article.pdf\" # Path to the PDF file\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# Asynchronously load pages\n",
    "pages = []\n",
    "async def load_pdf_pages():\n",
    "    async for page in loader.alazy_load():\n",
    "        pages.append(page)\n",
    "\n",
    "# Run the async function to load pages\n",
    "asyncio.run(load_pdf_pages())\n",
    "\n",
    "# Step 2: Define General Metadata for the Research Paper\n",
    "metadata = {\n",
    "    \"type\": \"research paper\",  # Type of document\n",
    "    \"title\": \"The risks associated with Artificial General Intelligence: A systematic review\",  #The title\n",
    "    \"source\": \"Journal of Experimental & Theoretical Artificial Intelligence\",  # Journal name\n",
    "    \"author(s)\": \", \".join([  # Convert list of authors to a comma-separated string\n",
    "        \"Scott McLean\", \"Gemma J. M. Read\", \"Jason Thompson\", \n",
    "        \"Chris Baber\", \"Neville A. Stanton\", \"Paul M. Salmon\"\n",
    "    ]),\n",
    "    \"abstract\": \"\"\"Artificial General intelligence (AGI) offers enormous benefits for humanity,\n",
    "    yet it also poses great risk. The aim of this systematic review was to\n",
    "    summarise the peer reviewed literature on the risks associated with AGI.\n",
    "    The review followed the Preferred Reporting Items for Systematic Reviews\n",
    "    and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed\n",
    "    eligible for inclusion. Article types included in the review were classified\n",
    "    as philosophical discussions, applications of modelling techniques, and\n",
    "    assessment of current frameworks and processes in relation to AGI. The\n",
    "    review identified a range of risks associated with AGI, including AGI\n",
    "    removing itself from the control of human owners/managers, being\n",
    "    given or developing unsafe goals, development of unsafe AGI, AGIs with\n",
    "    poor ethics, morals and values; inadequate management of AGI, and\n",
    "    existential risks. Several limitations of the AGI literature base were also\n",
    "    identified, including a limited number of peer reviewed articles and\n",
    "    modelling techniques focused on AGI risk, a lack of specific risk research\n",
    "    in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology.\n",
    "    Recommendations to address the identified issues with AGI risk research\n",
    "    are required to guide AGI design, implementation, and management\"\"\",  # Abstract\n",
    "    \"date\": \"August 2021\",  # Publication date\n",
    "    \"keywords\": \", \".join([\"AGI\", \"risk assessment\", \"AI safety\", \"Artificial Intelligence\"]),  # Keywords\n",
    "}\n",
    "\n",
    "# Step 3: Initialize the Text Splitter for Chunking\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "documents_with_metadata = []\n",
    "\n",
    "# Step 4: Split each page's content and add metadata\n",
    "for page in pages:\n",
    "    page_number = page.metadata.get(\"page\", \"Unknown\")  # Extract page number\n",
    "    \n",
    "    # Split text on each page into chunks\n",
    "    page_chunks = splitter.split_text(page.page_content)\n",
    "    \n",
    "    for chunk in page_chunks:\n",
    "        # Add metadata for each chunk\n",
    "        chunk_metadata = {\n",
    "            **metadata,  # General metadata for the document\n",
    "            \"page\": page_number  # Specific page number metadata\n",
    "        }\n",
    "        \n",
    "        # Create a LangChain Document for each chunk with metadata\n",
    "        langchain_doc = LangChainDocument(page_content=chunk, metadata=chunk_metadata)\n",
    "        documents_with_metadata.append(langchain_doc)\n",
    "\n",
    "# Step 5: Embed the Chunks in Chroma Vector Store\n",
    "vectorstore = Chroma.from_documents(documents=documents_with_metadata, \n",
    "                                    embedding=OpenAIEmbeddings(),\n",
    "                                    persist_directory=persist_directory)\n",
    "print(\"Successfully embedded the chunks in Chroma Vector Store.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents and Metadata:\n",
      "\n",
      "Document 1:\n",
      "Content: Roman Yampolskiy: So the problem of controlling AGI or superintelligence, in my opinion, is like the problem of creating a perpetual safety machine by analogy with a perpetual motion machine. It's impossible. Yeah, we may succeed and do a good job with GPT-5, 6, 7, but they just keep improving, learning, eventually self-modifying, interacting with the environment, interacting with malevolent actors. The difference between cybersecurity, narrow AI safety, and safety for general AI or superintelligence is that we don't get a second chance with cybersecurity. Somebody hacks your account; what's the big deal? You get a new password, a new credit card, you move on. Here, if we're talking about existential risks, you only get one chance. So you're really asking me what are the chances that we'll create the most complex software ever on the first try with zero bugs? And it will continue to have zero bugs for 100 years or more.\n",
      "Metadata: {'date': 'June 13, 2024', 'source': 'YouTube', 'speaker': 'Roman Yampolskiy', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\", 'speaker_role': 'Guest', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview'}\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "Content: Lex Fridman: Okay, so what's the S risk? What are the possible things that you're imagining with S risk? So mass suffering of humans, what are we talking about there? Caused by AGI?\n",
      "\n",
      "Roman Yampolskiy: So there are many malevolent actors we can talk about—psychopaths, crazies, hackers, doomsday cults. We know from history they tried killing everyone; they tried on purpose to cause the maximum amount of damage, terrorism. What if someone malevolent wants to torture all humans for as long as possible? You solve aging, so now you have functional immortality, and you just try to be as creative as you can.\n",
      "\n",
      "Lex Fridman: Do you think there are actually people in human history who tried to literally maximize human suffering? Just studying people who have done evil in the world, it seems that they think that they're doing good. It doesn't seem like they're trying to maximize suffering; they just cause a lot of suffering as a side effect of doing what they think is good.\n",
      "Metadata: {'date': 'June 13, 2024', 'source': 'YouTube', 'speaker': 'Lex Fridman', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.', 'speaker_role': 'Interviewer', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview'}\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "Content: Roman Yampolskiy: I think about a lot of things. So there is existential risk, where everyone's dead. There is suffering risk, where everyone wishes they were dead. We also have the idea of Ikigai risk, where we lost our meaning. The systems can be more creative; they can do all the jobs. It's not obvious what you have to contribute to a world where superintelligence exists. Of course, you can have all the variants you mentioned where we are safe, we are kept alive, but we are not in control. We are not deciding anything. We are like animals in a zoo. There are possibilities we can come up with as very smart humans, and then possibilities something a thousand times smarter can come up with for reasons we cannot comprehend.\n",
      "\n",
      "Lex Fridman: I would love to sort of dig into each of those X risk, S risk, and Ikigai risks. So can you linger on Ikigai? What is that?\n",
      "Metadata: {'date': 'June 13, 2024', 'source': 'YouTube', 'speaker': 'Lex Fridman', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.', 'speaker_role': 'Interviewer', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview'}\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 4:\n",
      "Content: Lex Fridman: See, I'm much more concerned about social engineering. So to me, AI's ability to do something in the physical world is like the lowest hanging fruit. The easiest set of methods is by just getting humans to do it. It's going to be much harder to be the kind of virus that takes over the minds of robots, where the robots are executing the commands. It just seems like humans social engineering of humans is much more likely.\n",
      "\n",
      "Roman Yampolskiy: That would be enough to bootstrap the whole process.\n",
      "\n",
      "Lex Fridman: Okay, just to linger on the term AGI. What to you is the difference between AGI and human-level intelligence?\n",
      "Metadata: {'date': 'June 13, 2024', 'source': 'YouTube', 'speaker': 'Lex Fridman', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.', 'speaker_role': 'Interviewer', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample query for retrieval (adjust the query based on your use case)\n",
    "sample_query = \"Will AGI pose existential risks to humanity?\"\n",
    "\n",
    "retrieved_docs = vectorstore.max_marginal_relevance_search(\n",
    "    query=sample_query,\n",
    "    num_results=5,\n",
    "    fetch_k=15,\n",
    "    filter={\"type\": \"interview\"}\n",
    ")\n",
    "\n",
    "# Inspect the retrieved documents and their metadata\n",
    "print(\"Retrieved Documents and Metadata:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i + 1}:\")\n",
    "    print(\"Content:\", doc.page_content)  # Show first 200 characters for context\n",
    "    print(\"Metadata:\", doc.metadata)  # Print all metadata fields\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain_community wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "docs = WikipediaLoader(query=\"Artificial General Intelligence\", load_max_docs=2).load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Artificial intelligence',\n",
       " 'summary': 'Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\\nSome high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nThe various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performable by a human on an at least equal level—is among the field\\'s long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s hundreds of billions of dollars were being invested in AI (known as the \"AI boom\"). The widespread use of AI in the 21st century exposed several unintended consequences and harms in the present and raised concerns about its risks and long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.',\n",
       " 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].metadata  # metadata of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial general intelligence (AGI) is a type of artificial intelligence (AI) that matches or surpasses human cognitive capabilities across a wide range of cognitive tasks. This contrasts with narrow AI, which is limited to specific tasks. Artificial superintelligence (ASI), on the other hand, refers to AGI that greatly exceeds human cognitive capabilities. AGI is considered one of the definitio'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:400]  # a part of the page content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk and embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Get the URL of the article\n",
    "url = \"https://time.com/7093792/ai-artificial-general-intelligence-risks/\"\n",
    "loader = WebBaseLoader([url])\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "\n",
    "#Define chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "#Split the document into chunks using the textsplitter\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "#Add metadata to each chunk\n",
    "metadata = {\n",
    "    \"title\": \"Silicon Valley Takes Artificial General Intelligence Seriously—Washington Must Too\",  #The title\n",
    "    \"journal\": \"Time Magazine\",  # Journal name\n",
    "    \"authors\": \"Daniel Colson\",  # Authors\n",
    "    \"summary\": \"\"\"highlights the urgent need for U.S. policymakers to address the rapid \n",
    "    advancements in Artificial General Intelligence (AGI). During a Senate \n",
    "    Judiciary Subcommittee hearing on September 17, 2024, whistleblowers from\n",
    "    leading AI companies, including OpenAI and Google, emphasized that AGI \n",
    "    is no longer a distant speculation but an impending reality. Helen Toner,\n",
    "    a former OpenAI board member, testified that companies like OpenAI,\n",
    "    Google, and Anthropic are earnestly pursuing AGI development. Former\n",
    "    OpenAI researcher William Saunders noted that these companies are \n",
    "    investing billions toward this goal. Senators Josh Hawley and \n",
    "    Richard Blumenthal expressed concern over the lack of regulatory \n",
    "    oversight, warning of potential risks such as cyberattacks, \n",
    "    economic upheaval, and even human extinction. The article \n",
    "    calls for immediate regulatory action, public engagement, \n",
    "    and the implementation of transparency and security mandates \n",
    "    to manage the societal impacts of AGI responsibly.\"\"\",  # Summary\n",
    "    \"date\": \"October 18th 2021\",  # Publication date\n",
    "    \n",
    "}\n",
    "\n",
    "# Initialize the list to store the documents with metadata\n",
    "documents_with_metadata = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Silicon Valley Takes AGI Seriously—Washington Should Too | TIMESign Up for Our Ideas Newsletter POVSubscribeSubscribeSectionsHomeU.S.PoliticsWorldHealthClimateFuture of Work by CharterBusinessTechEntertainmentIdeasScienceHistorySportsMagazineTIME 2030Next Generation LeadersTIME100 Leadership SeriesTIME StudiosVideoTIME100 TalksTIMEPiecesThe TIME VaultTIME for HealthTIME for KidsTIME EdgeTIMECO2Red Border: Branded Content by TIMECouponsPersonal Finance by TIME StampedShopping by TIME StampedJoin UsNewslettersSubscribeGive a GiftShop the TIME StoreTIME Cover StoreDigital MagazineCustomer CareUS & CanadaGlobal Help CenterReach OutCareersPress RoomContact the EditorsMedia KitReprints and PermissionsMoreAbout UsPrivacy PolicyYour Privacy RightsTerms of UseModern Slavery StatementSite MapConnect with UsPresented ByIdeasAI in FocusSilicon Valley Takes Artificial General Intelligence Seriously—Washington Must TooSilicon Valley Takes Artificial General Intelligence Seriously—Washington Must ...\n",
      "Metadata: {'source': 'https://time.com/7093792/ai-artificial-general-intelligence-risks/', 'title': 'Silicon Valley Takes AGI Seriously—Washington Should Too | TIME', 'description': \"Artificial generative intelligence is no longer a distant speculation—it's an impending reality that carries  enormous risk. \", 'language': 'en'}\n",
      "--------------------------------------------------\n",
      "Content: Must TooSilicon Valley Takes Artificial General Intelligence Seriously—Washington Must Too5 minute readGetty ImagesIdeasBy Daniel ColsonOctober 18, 2024 7:10 AM EDTDaniel Colson is the Executive Director of the AI Policy Institute.Artificial General Intelligence—machines that can learn and perform any cognitive task that a human can—has long been relegated to the realm of science fiction. But recent developments show that AGI is no longer a distant speculation; it’s an impending reality that demands our immediate attention.On Sept. 17, during a Senate Judiciary Subcommittee hearing titled “Oversight of AI: Insiders’ Perspectives,” whistleblowers from leading AI companies sounded the alarm on the rapid advancement toward AGI and the glaring lack of oversight. Helen Toner, a former board member of OpenAI and director of strategy at Georgetown University’s Center for Security and Emerging Technology, testified that, “The biggest disconnect that I see between AI insider perspectives and ...\n",
      "Metadata: {'source': 'https://time.com/7093792/ai-artificial-general-intelligence-risks/', 'title': 'Silicon Valley Takes AGI Seriously—Washington Should Too | TIME', 'description': \"Artificial generative intelligence is no longer a distant speculation—it's an impending reality that carries  enormous risk. \", 'language': 'en'}\n",
      "--------------------------------------------------\n",
      "Content: Technology, testified that, “The biggest disconnect that I see between AI insider perspectives and public perceptions of AI companies is when it comes to the idea of artificial general intelligence.” She continued that leading AI companies such as OpenAI, Google, and Anthropic are “treating building AGI as an entirely serious goal.”Toner’s co-witness William Saunders—a former researcher at OpenAI who recently resigned after losing faith in OpenAI acting responsibly—echoed similar sentiments to Toner, testifying that, “Companies like OpenAI are working towards building artificial general intelligence” and that “they are raising billions of dollars towards this goal.”Read More: When Might AI Outsmart Us? It Depends Who You AskAll three leading AI labs—OpenAI, Anthropic, and Google DeepMind—are more or less explicit about their AGI goals. OpenAI’s mission states: “To ensure that artificial general intelligence—by which we mean highly autonomous systems that outperform humans at most ...\n",
      "Metadata: {'source': 'https://time.com/7093792/ai-artificial-general-intelligence-risks/', 'title': 'Silicon Valley Takes AGI Seriously—Washington Should Too | TIME', 'description': \"Artificial generative intelligence is no longer a distant speculation—it's an impending reality that carries  enormous risk. \", 'language': 'en'}\n",
      "--------------------------------------------------\n",
      "Content: general intelligence—by which we mean highly autonomous systems that outperform humans at most economically valuable work—benefits all of humanity.” Anthropic focuses on “building reliable, interpretable, and steerable AI systems,” aiming for “safe AGI.” Google DeepMind aspires “to solve intelligence” and then to use the resultant AI systems “to solve everything else,” with co-founder Shane Legg stating unequivocally that he expects “human-level AI will be passed in the mid-2020s.” New entrants into the AI race, such as Elon Musk’s xAI and Ilya Sutskever’s Safe Superintelligence Inc., are similarly focused on AGI.Policymakers in Washington have mostly dismissed AGI as either marketing hype or a vague metaphorical device not meant to be taken literally. But last month’s hearing might have broken through in a way that previous discourse of AGI has not. Senator Josh Hawley (R-MO), Ranking Member of the subcommittee, commented that the witnesses are “folks who have been inside [AI] ...\n",
      "Metadata: {'source': 'https://time.com/7093792/ai-artificial-general-intelligence-risks/', 'title': 'Silicon Valley Takes AGI Seriously—Washington Should Too | TIME', 'description': \"Artificial generative intelligence is no longer a distant speculation—it's an impending reality that carries  enormous risk. \", 'language': 'en'}\n",
      "--------------------------------------------------\n",
      "Content: Member of the subcommittee, commented that the witnesses are “folks who have been inside [AI] companies, who have worked on these technologies, who have seen them firsthand, and I might just observe don’t have quite the vested interest in painting that rosy picture and cheerleading in the same way that [AI company] executives have.”Senator Richard Blumenthal (D-CT), the subcommittee Chair, was even more direct. “The idea that AGI might in 10 or 20 years be smarter or at least as smart as human beings is no longer that far out in the future. It’s very far from science fiction. It’s here and now—one to three years has been the latest prediction,” he said. He didn’t mince words about where responsibility lies: “What we should learn from social media, that experience is, don’t trust Big Tech.”The apparent shift in Washington reflects public opinion that has been more willing to entertain the possibility of AGI’s imminence. In a July 2023 survey conducted by the AI Policy Institute, the ...\n",
      "Metadata: {'source': 'https://time.com/7093792/ai-artificial-general-intelligence-risks/', 'title': 'Silicon Valley Takes AGI Seriously—Washington Should Too | TIME', 'description': \"Artificial generative intelligence is no longer a distant speculation—it's an impending reality that carries  enormous risk. \", 'language': 'en'}\n",
      "--------------------------------------------------\n",
      "Content: the possibility of AGI’s imminence. In a July 2023 survey conducted by the AI Policy Institute, the majority of Americans said they thought AGI would be developed “within the next 5 years.” Some 82% of respondents also said we should “go slowly and deliberately” in AI development.That’s because the stakes are astronomical. Saunders detailed that AGI could lead to cyberattacks or the creation of “novel biological weapons,” and Toner warned that many leading AI figures believe that in a worst-case scenario AGI “could lead to literal human extinction.”Despite these stakes, the U.S. has instituted almost no regulatory oversight over the companies racing toward AGI. So where does this leave us?First, Washington needs to start taking AGI seriously. The potential risks are too great to ignore. Even in a good scenario, AGI could upend economies and displace millions of jobs, requiring society to adapt. In a bad scenario, AGI could become uncontrollable.Second, we must establish regulatory ...\n",
      "Metadata: {'source': 'https://time.com/7093792/ai-artificial-general-intelligence-risks/', 'title': 'Silicon Valley Takes AGI Seriously—Washington Should Too | TIME', 'description': \"Artificial generative intelligence is no longer a distant speculation—it's an impending reality that carries  enormous risk. \", 'language': 'en'}\n",
      "--------------------------------------------------\n",
      "Content: to adapt. In a bad scenario, AGI could become uncontrollable.Second, we must establish regulatory guardrails for powerful AI systems. Regulation should involve government transparency into what’s going on with the most powerful AI systems that are being created by tech companies. Government transparency will reduce the chances that society is caught flat-footed by a tech company developing AGI before anyone else is expecting. And mandated security measures are needed to prevent U.S. adversaries and other bad actors from stealing AGI systems from U.S. companies. These light-touch measures would be sensible even if AGI weren’t a possibility, but the prospect of AGI heightens their importance.Read More: What an American Approach to AI Regulation Should Look LikeIn a particularly concerning part of Saunders’ testimony, he said that during his time at OpenAI there were long stretches where he or hundreds of other employees would be able to “bypass access controls and steal the company’s ...\n",
      "Metadata: {'source': 'https://time.com/7093792/ai-artificial-general-intelligence-risks/', 'title': 'Silicon Valley Takes AGI Seriously—Washington Should Too | TIME', 'description': \"Artificial generative intelligence is no longer a distant speculation—it's an impending reality that carries  enormous risk. \", 'language': 'en'}\n",
      "--------------------------------------------------\n",
      "Content: he or hundreds of other employees would be able to “bypass access controls and steal the company’s most advanced AI systems, including GPT-4.” This lax attitude toward security is bad enough for U.S. competitiveness today, but it is an absolutely unacceptable way to treat systems on the path to AGI. The comments were another powerful reminder that tech companies cannot be trusted to self-regulate.Finally, public engagement is essential. AGI isn’t just a technical issue; it’s a societal one. The public must be informed and involved in discussions about how AGI could impact all of our lives.No one knows how long we have until AGI—what Senator Blumenthal referred to as “the 64 billion dollar question”—but the window for action may be rapidly closing. Some AI figures including Saunders think it may be in as little as three years. Ignoring the potentially imminent challenges of AGI won’t make them disappear. It’s time for policymakers to begin to get their heads out of the cloud. More ...\n",
      "Metadata: {'source': 'https://time.com/7093792/ai-artificial-general-intelligence-risks/', 'title': 'Silicon Valley Takes AGI Seriously—Washington Should Too | TIME', 'description': \"Artificial generative intelligence is no longer a distant speculation—it's an impending reality that carries  enormous risk. \", 'language': 'en'}\n",
      "--------------------------------------------------\n",
      "Content: make them disappear. It’s time for policymakers to begin to get their heads out of the cloud. More Must-Reads from TIMEHow Donald Trump WonThe Best Inventions of 2024Why Sleep Is the Key to Living LongerHow to Break 8 Toxic Communication HabitsNicola Coughlan Bet on Herself—And WonWhat It’s Like to Have Long COVID As a Kid22 Essential Works of Indigenous CinemaMeet TIME's Newest Class of Next Generation LeadersContact us at letters@time.comTIME Ideas hosts the world's leading voices, providing commentary on events in news, society, and culture. We welcome outside contributions. Opinions expressed do not necessarily reflect the views of TIME editors.Edit PostHomeU.S.PoliticsWorldHealthBusinessTechPersonal Finance by TIME StampedShopping by TIME StampedFuture of Work by CharterEntertainmentIdeasScienceHistorySportsMagazineThe TIME VaultTIME For KidsTIMECO2CouponsTIME EdgeVideoMastheadNewslettersSubscribeDigital MagazineGive a GiftShop the TIME StoreCareersModern Slavery StatementPress ...\n",
      "Metadata: {'source': 'https://time.com/7093792/ai-artificial-general-intelligence-risks/', 'title': 'Silicon Valley Takes AGI Seriously—Washington Should Too | TIME', 'description': \"Artificial generative intelligence is no longer a distant speculation—it's an impending reality that carries  enormous risk. \", 'language': 'en'}\n",
      "--------------------------------------------------\n",
      "Content: MagazineGive a GiftShop the TIME StoreCareersModern Slavery StatementPress RoomTIME StudiosU.S. & Canada Customer CareGlobal Help CenterContact the EditorsReprints and PermissionsSite MapMedia KitSupplied Partner ContentAbout Us© 2024 TIME USA, LLC. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Service, Privacy Policy (Your Privacy Rights) and Do Not Sell or Share My Personal Information.TIME may receive compensation for some links to products and services on this website. Offers may be subject to change without notice. ...\n",
      "Metadata: {'source': 'https://time.com/7093792/ai-artificial-general-intelligence-risks/', 'title': 'Silicon Valley Takes AGI Seriously—Washington Should Too | TIME', 'description': \"Artificial generative intelligence is no longer a distant speculation—it's an impending reality that carries  enormous risk. \", 'language': 'en'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Take a look at splits\n",
    "for doc in splits:\n",
    "    print(\"Content:\", doc.page_content[:], \"...\")  # Print first 200 characters for brevity\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Silicon Valley Takes AGI Seriously—Washington Should Too | TIMESign Up for Our Ideas Newsletter POVSubscribeSubscribeSectionsHomeU.S.PoliticsWorldHealthClimateFuture of Work by CharterBusinessTechEntertainmentIdeasScienceHistorySportsMagazineTIME 2030Next Generation LeadersTIME100 Leadership SeriesTIME StudiosVideoTIME100 TalksTIMEPiecesThe TIME VaultTIME for HealthTIME for KidsTIME EdgeTIMECO2Red Border: Branded Content by TIMECouponsPersonal Finance by TIME StampedShopping by TIME StampedJoin UsNewslettersSubscribeGive a GiftShop the TIME StoreTIME Cover StoreDigital MagazineCustomer CareUS & CanadaGlobal Help CenterReach OutCareersPress RoomContact the EditorsMedia KitReprints and PermissionsMoreAbout UsPrivacy PolicyYour Privacy RightsTerms of UseModern Slavery StatementSite MapConnect with UsPresented ByIdeasAI in FocusSilicon Valley Takes Artificial General Intelligence Seriously—Washington Must TooSilicon Valley Takes Artificial General Intelligence Seriously—Washington Must Too5 minute readGetty ImagesIdeasBy Daniel ColsonOctober 18, 2024 7:10 AM EDTDaniel Colson is the Executive Director of the AI Policy Institute.Artificial General Intelligence—machines that can learn and perform any cognitive task that a human can—has long been relegated to the realm of science fiction. But recent developments show that AGI is no longer a distant speculation; it’s an impending reality that demands our immediate attention.On Sept. 17, during a Senate Judiciary Subcommittee hearing titled “Oversight of AI: Insiders’ Perspectives,” whistleblowers from leading AI companies sounded the alarm on the rapid advancement toward AGI and the glaring lack of oversight. Helen Toner, a former board member of OpenAI and director of strategy at Georgetown University’s Center for Security and Emerging Technology, testified that, “The biggest disconnect that I see between AI insider perspectives and public perceptions of AI companies is when it comes to the idea of artificial general intelligence.” She continued that leading AI companies such as OpenAI, Google, and Anthropic are “treating building AGI as an entirely serious goal.”Toner’s co-witness William Saunders—a former researcher at OpenAI who recently resigned after losing faith in OpenAI acting responsibly—echoed similar sentiments to Toner, testifying that, “Companies like OpenAI are working towards building artificial general intelligence” and that “they are raising billions of dollars towards this goal.”Read More: When Might AI Outsmart Us? It Depends Who You AskAll three leading AI labs—OpenAI, Anthropic, and Google DeepMind—are more or less explicit about their AGI goals. OpenAI’s mission states: “To ensure that artificial general intelligence—by which we mean highly autonomous systems that outperform humans at most economically valuable work—benefits all of humanity.” Anthropic focuses on “building reliable, interpretable, and steerable AI systems,” aiming for “safe AGI.” Google DeepMind aspires “to solve intelligence” and then to use the resultant AI systems “to solve everything else,” with co-founder Shane Legg stating unequivocally that he expects “human-level AI will be passed in the mid-2020s.” New entrants into the AI race, such as Elon Musk’s xAI and Ilya Sutskever’s Safe Superintelligence Inc., are similarly focused on AGI.Policymakers in Washington have mostly dismissed AGI as either marketing hype or a vague metaphorical device not meant to be taken literally. But last month’s hearing might have broken through in a way that previous discourse of AGI has not. Senator Josh Hawley (R-MO), Ranking Member of the subcommittee, commented that the witnesses are “folks who have been inside [AI] companies, who have worked on these technologies, who have seen them firsthand, and I might just observe don’t have quite the vested interest in painting that rosy picture and cheerleading in the same way that [AI company] executives have.”Senator Richard Blumenthal (D-CT), the subcommittee Chair, was even more direct. “The idea that AGI might in 10 or 20 years be smarter or at least as smart as human beings is no longer that far out in the future. It’s very far from science fiction. It’s here and now—one to three years has been the latest prediction,” he said. He didn’t mince words about where responsibility lies: “What we should learn from social media, that experience is, don’t trust Big Tech.”The apparent shift in Washington reflects public opinion that has been more willing to entertain the possibility of AGI’s imminence. In a July 2023 survey conducted by the AI Policy Institute, the majority of Americans said they thought AGI would be developed “within the next 5 years.” Some 82% of respondents also said we should “go slowly and deliberately” in AI development.That’s because the stakes are astronomical. Saunders detailed that AGI could lead to cyberattacks or the creation of “novel biological weapons,” and Toner warned that many leading AI figures believe that in a worst-case scenario AGI “could lead to literal human extinction.”Despite these stakes, the U.S. has instituted almost no regulatory oversight over the companies racing toward AGI. So where does this leave us?First, Washington needs to start taking AGI seriously. The potential risks are too great to ignore. Even in a good scenario, AGI could upend economies and displace millions of jobs, requiring society to adapt. In a bad scenario, AGI could become uncontrollable.Second, we must establish regulatory guardrails for powerful AI systems. Regulation should involve government transparency into what’s going on with the most powerful AI systems that are being created by tech companies. Government transparency will reduce the chances that society is caught flat-footed by a tech company developing AGI before anyone else is expecting. And mandated security measures are needed to prevent U.S. adversaries and other bad actors from stealing AGI systems from U.S. companies. These light-touch measures would be sensible even if AGI weren’t a possibility, but the prospect of AGI heightens their importance.Read More: What an American Approach to AI Regulation Should Look LikeIn a particularly concerning part of Saunders’ testimony, he said that during his time at OpenAI there were long stretches where he or hundreds of other employees would be able to “bypass access controls and steal the company’s most advanced AI systems, including GPT-4.” This lax attitude toward security is bad enough for U.S. competitiveness today, but it is an absolutely unacceptable way to treat systems on the path to AGI. The comments were another powerful reminder that tech companies cannot be trusted to self-regulate.Finally, public engagement is essential. AGI isn’t just a technical issue; it’s a societal one. The public must be informed and involved in discussions about how AGI could impact all of our lives.No one knows how long we have until AGI—what Senator Blumenthal referred to as “the 64 billion dollar question”—but the window for action may be rapidly closing. Some AI figures including Saunders think it may be in as little as three years. Ignoring the potentially imminent challenges of AGI won’t make them disappear. It’s time for policymakers to begin to get their heads out of the cloud. More Must-Reads from TIMEHow Donald Trump WonThe Best Inventions of 2024Why Sleep Is the Key to Living LongerHow to Break 8 Toxic Communication HabitsNicola Coughlan Bet on Herself—And WonWhat It’s Like to Have Long COVID As a Kid22 Essential Works of Indigenous CinemaMeet TIME's Newest Class of Next Generation LeadersContact us at letters@time.comTIME Ideas hosts the world's leading voices, providing commentary on events in news, society, and culture. We welcome outside contributions. Opinions expressed do not necessarily reflect the views of TIME editors.Edit PostHomeU.S.PoliticsWorldHealthBusinessTechPersonal Finance by TIME StampedShopping by TIME StampedFuture of Work by CharterEntertainmentIdeasScienceHistorySportsMagazineThe TIME VaultTIME For KidsTIMECO2CouponsTIME EdgeVideoMastheadNewslettersSubscribeDigital MagazineGive a GiftShop the TIME StoreCareersModern Slavery StatementPress RoomTIME StudiosU.S. & Canada Customer CareGlobal Help CenterContact the EditorsReprints and PermissionsSite MapMedia KitSupplied Partner ContentAbout Us© 2024 TIME USA, LLC. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Service, Privacy Policy (Your Privacy Rights) and Do Not Sell or Share My Personal Information.TIME may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 1000 characters of article body\n",
    "documents[0].page_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article #1 (Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Define the URL and fetch HTML content\n",
    "url = \"https://time.com/7093792/ai-artificial-general-intelligence-risks/\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "# Step 2: Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 3: Locate the main article body\n",
    "# Look for an element that contains the main article text\n",
    "# You may need to adjust these selectors based on the HTML structure of the Time website\n",
    "\n",
    "article_body = soup.find(\"div\", id=\"article-body-main\")  # Adjust if needed\n",
    "if not article_body:\n",
    "    article_body = soup.find(\"article\")  # Use <article> tag as a fallback\n",
    "\n",
    "# Step 4: Remove unwanted elements, such as ads or related links\n",
    "for unwanted in article_body.find_all([\"aside\", \"footer\", \"nav\", \"script\", \"style\", \"figure\", \"div\"]):\n",
    "    unwanted.decompose()  # This removes the unwanted tags and their content\n",
    "\n",
    "# Step 5: Extract and clean the text\n",
    "body_text = article_body.get_text()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to chunk the text into smaller parts and add metadata\n",
    "\n",
    "# Define the metadata\n",
    "metadata = {\n",
    "    \"type\": \"article\",\n",
    "    \"title\": \"Silicon Valley Takes Artificial General Intelligence Seriously—Washington Must Too\",\n",
    "    \"source\": \"Time Magazine\",\n",
    "    \"author(s)\": \"Daniel Colson\",\n",
    "    \"date\": \"October 18 2024\",\n",
    "    \"summary\": \"\"\"highlights the urgent need for U.S. policymakers to address the rapid \n",
    "    advancements in Artificial General Intelligence (AGI). During a Senate \n",
    "    Judiciary Subcommittee hearing on September 17, 2024, whistleblowers from\n",
    "    leading AI companies, including OpenAI and Google, emphasized that AGI \n",
    "    is no longer a distant speculation but an impending reality. Helen Toner,\n",
    "    a former OpenAI board member, testified that companies like OpenAI,\n",
    "    Google, and Anthropic are earnestly pursuing AGI development. Former\n",
    "    OpenAI researcher William Saunders noted that these companies are \n",
    "    investing billions toward this goal. Senators Josh Hawley and \n",
    "    Richard Blumenthal expressed concern over the lack of regulatory \n",
    "    oversight, warning of potential risks such as cyberattacks, \n",
    "    economic upheaval, and even human extinction. The article \n",
    "    calls for immediate regulatory action, public engagement, \n",
    "    and the implementation of transparency and security mandates \n",
    "    to manage the societal impacts of AGI responsibly.\"\"\",\n",
    "}\n",
    "\n",
    "# Ensure body_text is a string\n",
    "body_text = str(body_text)\n",
    "\n",
    "# First create a LangChain Document from the extracted text\n",
    "document = LangChainDocument(page_content=body_text, metadata=metadata)\n",
    "\n",
    "#Define chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "#Split the document into chunks using the textsplitter\n",
    "chunks = text_splitter.split_documents([document])\n",
    "\n",
    "#Embed the chunks into Chroma vectorstore\n",
    "vectorstore = Chroma.from_documents(documents=chunks, \n",
    "                                    embedding=OpenAIEmbeddings(),\n",
    "                                    persist_directory=persist_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article #2 (Futurism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Define the URL and fetch HTML content\n",
    "url = \"https://futurism.com/the-byte/sam-altman-excited-agi-2025\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "# Step 2: Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 3: Locate the main article body\n",
    "# Look for an element that contains the main article text\n",
    "# You may need to adjust these selectors based on the HTML structure of the Time website\n",
    "\n",
    "article_body = soup.find(\"div\", id=\"incArticle\")  # Adjust if needed\n",
    "if not article_body:\n",
    "    article_body = soup.find(\"article\")  # Use <article> tag as a fallback\n",
    "\n",
    "# Step 4: Remove unwanted elements, such as ads or related links\n",
    "for unwanted in article_body.find_all([\"aside\", \"footer\", \"nav\", \"script\", \"style\", \"figure\"]):\n",
    "    unwanted.decompose()  # This removes the unwanted tags and their content\n",
    "\n",
    "# Step 5: Extract and clean the text\n",
    "body_text = article_body.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the text as a word document to look at if you want.\n",
    "\"\"\" from docx import Document\n",
    "\n",
    "# Create a new Document\n",
    "doc = Document()\n",
    "\n",
    "# Add the content of body_text to the document\n",
    "doc.add_paragraph(body_text)\n",
    "\n",
    "# Save the document\n",
    "doc.save(\"body_text.docx\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document as LangChainDocument\n",
    "\n",
    "# Now we need to chunk the text into smaller parts and add metadata\n",
    "\n",
    "# Define the metadata\n",
    "metadata = {\n",
    "    \"type\": \"article\",\n",
    "    \"title\": \"SAM ALTMAN SAYS THE MAIN THING HE’S EXCITED ABOUT NEXT YEAR IS ACHIEVING AGI\",\n",
    "    \"source\": \"Futurism\",\n",
    "    \"author(s)\": \"Noor Al-Sibai\",\n",
    "    \"date\": \"November 11 2024\",\n",
    "    \"summary\": \"\"\"Article talks about how Sam Altman predicts there will be AGI in 2025.\n",
    "    It is not specified exactly how that will be achieved, or where in the progress\n",
    "    they (OpenAI) are.\"\"\",\n",
    "}\n",
    "\n",
    "# Ensure body_text is a string\n",
    "body_text = str(body_text)\n",
    "\n",
    "# First create a LangChain Document from the extracted text\n",
    "document = LangChainDocument(page_content=body_text, metadata=metadata)\n",
    "\n",
    "#Define chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "#Split the document into chunks using the textsplitter\n",
    "chunks = text_splitter.split_documents([document])\n",
    "\n",
    "#Embed the chunks into Chroma vectorstore\n",
    "vectorstore = Chroma.from_documents(documents=chunks, \n",
    "                                    embedding=OpenAIEmbeddings(),\n",
    "                                    persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article #3 (FUCK OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Get the URL of the article\n",
    "#url = \"https://www.artificialintelligence-news.com/news/asi-alliance-launches-airis-learns-minecraft/\"\n",
    "#loader = WebBaseLoader([url])\n",
    "#document = loader.load()  # Get the first document\n",
    "\n",
    "html_content = document[0].page_content \n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nASI Alliance launches AIRIS that ‘learns’ in Minecraft\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n \\n \\n\\n\\n \\n \\n\\n\\n\\n\\n\\n \\n \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n                    ARTICLE\\n                \\n \\n\\nLOG IN\\n\\n \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNews\\nCategories\\n\\nApplications\\n\\nChatbots\\nFace Recognition\\nVirtual Assistants\\nVoice Recognition\\n\\n\\nCompanies\\n\\nAmazon\\nApple\\nGoogle\\nMeta (Facebook)\\nMicrosoft\\nNVIDIA\\n\\n\\nDeep & Reinforcement Learning\\nEnterprise\\nEthics & Society\\nIndustries\\n\\nEnergy\\nEntertainment & Retail\\nGaming\\nHealthcare\\nManufacturing\\n\\n\\nLegislation & Government\\nMachine Learning\\nPrivacy\\nResearch\\nRobotics\\nSecurity\\nSurveillance\\n\\n\\nEvents\\nWebinars & Resources\\nWork With Us\\n\\nPublish an article or press release\\nAdvertise on AI News\\nContact Us\\n\\n\\nSubscribe / Login\\nMore TechForge News\\n\\nThe Block\\nCloud Tech News\\nDeveloper News\\nEdge Computing News\\nIoT News\\nMarketing Tech News\\nTech HQ\\nTech Wire Asia\\nTelecoms Tech News\\n\\n\\nUpcoming Events\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n\\n\\n\\n\\n\\n\\n\\nNews\\nCategories\\n\\nApplications\\n\\nChatbots\\nFace Recognition\\nVirtual Assistants\\nVoice Recognition\\n\\n\\nCompanies\\n\\nAmazon\\nApple\\nGoogle\\nMeta (Facebook)\\nMicrosoft\\nNVIDIA\\n\\n\\nDeep & Reinforcement Learning\\nEnterprise\\nEthics & Society\\nIndustries\\n\\nEnergy\\nEntertainment & Retail\\nGaming\\nHealthcare\\nManufacturing\\n\\n\\nLegislation & Government\\nMachine Learning\\nPrivacy\\nResearch\\nRobotics\\nSecurity\\nSurveillance\\n\\n\\nEvents\\nWebinars & Resources\\nWork With Us\\n\\nPublish an article or press release\\nAdvertise on AI News\\nContact Us\\n\\n\\nSubscribe / Login\\nMore TechForge News\\n\\nThe Block\\nCloud Tech News\\nDeveloper News\\nEdge Computing News\\nIoT News\\nMarketing Tech News\\nTech HQ\\nTech Wire Asia\\nTelecoms Tech News\\n\\n\\nUpcoming Events\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nASI Alliance launches AIRIS that ‘learns’ in Minecraft\\n\\n\\n\\n\\n \\n\\n\\n\\nAbout the Author\\nBy Ryan Daws |\\n        November 6, 2024\\nhttps://twitter.com/gadget_ry \\n                            Categories:\\n                                    AGI,\\n                                    Applications,\\n                                    Artificial Intelligence,\\n                                    Companies,\\n                                    Gaming,\\n                                    Machine Learning,\\n                        \\n\\n\\n\\n Ryan Daws is a senior editor at TechForge Media with over a decade of experience in crafting compelling narratives and making complex topics accessible. His articles and interviews with industry leaders have earned him recognition as a key influencer by organisations like Onalytica. Under his leadership, publications have been praised by analyst firms such as Forrester for their excellence and performance. Connect with him on X (@gadget_ry) or Mastodon (@gadgetry@techhub.social)\\n\\n\\n\\n\\n\\n\\n\\n\\nThe ASI Alliance has introduced AIRIS (Autonomous Intelligent Reinforcement Inferred Symbolism) that “learns” within the popular game, Minecraft.\\nAIRIS represents the first proto-AGI (Artificial General Intelligence) to harness a comprehensive tech stack across the alliance.\\nSingularityNET, founded by renowned AI researcher Dr Ben Goertzel, uses agent technology from Fetch.ai, incorporates Ocean Data for long-term memory capabilities, and is soon expected to integrate CUDOS Compute infrastructure for scalable processing power.\\n“AIRIS is a significant step in the direction of practical, scalable neural-symbolic learning, and – alongside its already powerful and valuable functionality – it illustrates several general points about neural-symbolic systems, such as their ability to learn precise generalisable conclusions from small amounts of data,” explains Goertzel.\\nAccording to the company, this alliance-driven procedure propels AIRIS towards AGI—crafting one of the first intelligent systems with autonomous and adaptive learning that holds practical applications for real-world scenarios.\\nAIRIS’ learning mechanisms\\nAIRIS is crafted to enhance its understanding by interacting directly with its environment, venturing beyond the traditional AI limitations that depend on predefined rules or vast datasets. Instead, AIRIS evolves through observation, experimentation, and continual refinement of its unique “rule set.”\\nThis system facilitates a profound level of problem-solving and contextual comprehension, with its implementation in Minecraft setting a new benchmark for AI interaction with both digital and tangible landscapes.\\n\\n👀 pic.twitter.com/jTeQFulzFJ— Artificial Superintelligence Alliance (@ASI_Alliance) November 5, 2024\\n\\nShifting from a controlled 2D grid to the sophisticated 3D world of Minecraft, AIRIS faced numerous challenges—including terrain navigation and adaptive problem-solving in a dynamic environment. This transition underscores AIRIS’ autonomy in navigation, exploration, and learning.\\nThe AIRIS Minecraft Agent distinguishes itself from other AI entities through several key features:\\n\\nDynamic navigation: AIRIS initially evaluates its milieu to formulate movement strategies, adapting to new environments in real-time. Its capabilities include manoeuvring around obstacles, jumping over barriers, and anticipating reactions to varied terrains.\\n\\n\\nObstacle adaptation: It learns to navigate around impediments like cliffs and forested areas, refining its rule set with every new challenge to avoid redundant errors and minimise needless trial-and-error efforts.\\n\\n\\nEfficient pathfinding: Via continuous optimisation, AIRIS advances from initially complex navigation paths to streamlined, direct routes as it “comprehends” Minecraft dynamics.\\n\\n\\nReal-time environmental adaptation: Contrasting with conventional reinforcement learning systems that demand extensive retraining for new environments, AIRIS adapts immediately to unfamiliar regions, crafting new rules based on partial observations dynamically.\\n\\nAIRIS’ adeptness in dealing with fluctuating terrains, including water bodies and cave systems, introduces sophisticated rule refinement founded on hands-on experience. Additionally, AIRIS boasts optimised computational efficiency—enabling real-time management of complex rules without performance compromises.\\nFuture applications\\nMinecraft serves as an excellent launchpad for AIRIS’ prospective applications, establishing a solid foundation for expansive implementations:\\n\\nEnhanced object interaction: Forthcoming stages will empower AIRIS to engage more profoundly with its surroundings, improving capabilities in object manipulation, construction, and even crafting. This development will necessitate AIRIS to develop a more refined decision-making framework for contextual tasks.\\n\\n\\nSocial AI collaboration: Plans are underway to incorporate AIRIS in multi-agent scenarios, where agents learn, interact, and fulfil shared objectives, simulating real-world social dynamics and problem-solving collaboratively.\\n\\n\\nAbstract and strategic reasoning: Expanded developments will enhance AIRIS’s reasoning, enabling it to tackle complex goals such as resource management and prioritisation, moving beyond basic navigation towards strategic gameplay.\\n\\nThe transition of AIRIS to 3D environments signifies a pivotal advancement in the ASI Alliance’s mission to cultivate AGI. Through AIRIS’s achievements in navigating and learning within Minecraft, the ASI Alliance aspires to expedite its deployment in the real world, pioneering applications for autonomous robots, intelligent home assistants, and other systems requiring adaptive learning and problem-solving capacities.\\nBerick Cook, AI Developer at SingularityNET and creator of AIRIS, said: “AIRIS is a whole new way of approaching the problem of machine learning. We are only just beginning to explore its capabilities. We are excited to see how we can apply it to problems that have posed a significant challenge for traditional reinforcement learning.\\n“The most important aspect of AIRIS to me is its transparency and explainability. Moving away from ‘Black Box’ AI represents a significant leap forward in the pursuit of safe, ethical, and beneficial AI.”\\nThe innovative approach to AI evident in AIRIS – emphasising self-directed learning and continuous rule refinement – lays the foundation for AI systems capable of independent functioning in unpredictable real-world environments. Minecraft’s intricate ecosystem enables the system to hone its skills within a controlled yet expansive virtual setting, effectively bridging the divide between simulation and reality.\\nThe AIRIS Minecraft Agent represents the inaugural tangible step towards an AI that learns from, adapts to and makes autonomous decisions about its environment. This accomplishment illustrates the potential of such technology to re-envision AI’s role across various industries.\\n(Image by SkyeWeste)\\nSee also: SingularityNET bets on supercomputer network to deliver AGI\\n\\nWant to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security & Cloud Expo.\\nExplore other upcoming enterprise technology events and webinars powered by TechForge here.\\nTags: agi, ai, airis, artificial intelligence, ben goertzel, fetch ai, fetch.ai, learning, singularitynet\\n\\n\\n\\n\\n\\n\\nView Comments\\n\\n\\nLeave a comment\\n\\n\\n\\n\\nLeave a Reply Cancel replyYou must be logged in to post a comment. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nJoin our community\\nCreate your free account now to access all our premium content and recieve the latest tech news to your inbox.\\nSUBSCRIBE NOW\\n\\nLATEST ARTICLES \\n\\n\\n\\n\\n\\nUnderstanding AI’s impact on the workforce\\n\\n\\nEthics & Society\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCentralised AI is dangerous: how can we stop it?\\n\\n\\nEthics & Society\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nASI Alliance launches AIRIS that ‘learns’ in Minecraft\\n\\n\\nAGI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAI hallucinations gone wrong as Alaska uses fake stats in policy\\n\\n\\nArtificial Intelligence\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWestern drivers remain sceptical of in-vehicle AI\\n\\n\\nApplications\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nOther NewsArtificial Intelligence News\\nBlockchain News\\nCloud Computing News\\nDeveloper News\\nEdge Computing News\\nIoT News\\nTech HQ\\nTech Wire Asia\\nTelecoms News\\nView all TechForge Publications\\nCookie Policy (UK)\\n \\n\\nEventsBlockchain & AI Expo\\nAI & Big Data Expo\\n#DMWF Digital Marketing World Forum\\nDeveloper Events\\nIoT Events\\nMarketing Events\\nTelecoms Events\\n \\n\\nCategoriesDeep & Reinforcement LearningMachine LearningRoboticsVoice Recognition \\n\\n\\n\\n\\n\\n \\n\\nAI News provides artificial intelligence news and jobs, industry analysis and digital media insight around numerous marketing disciplines; mobile strategy, email marketing, SEO, analytics, social media and much more.\\nPlease follow this link for our privacy policy.\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nCopyright © 2024 AI News. All Rights\\n                Reserved.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoginUsername(Required) Password(Required) \\n\\nRemember Me\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nForgot Password \\n\\nNot subscribed / a member yet?\\n\\nREGISTER HERE TO SUBSCRIBE NOW\\n\\n\\n\\n\\nRegister\\n\\n\\n\\n\\n\\n\\n\"*\" indicates required fields\\n\\nPersonal DetailsFirst Names* Last Name* Interests*AIBig DataBlockchainCloudCyber SecurityData CentresDevOpsDigital TransformationEdge ComputingEnterpriseFutureworksHigh Performance ComputingIoTMarketing TechQuantum ComputingVR/AR/XRCountry*United KingdomUnited StatesAfghanistanAlbaniaAlgeriaAmerican SamoaAndorraAngolaAnguillaAntarcticaAntigua and BarbudaArgentinaArmeniaArubaAustraliaAustriaAzerbaijanBahamasBahrainBangladeshBarbadosBelarusBelgiumBelizeBeninBermudaBhutanBoliviaBonaire, Sint Eustatius and SabaBosnia and HerzegovinaBotswanaBouvet IslandBrazilBritish Indian Ocean TerritoryBrunei DarussalamBulgariaBurkina FasoBurundiCambodiaCameroonCanadaCape VerdeCayman IslandsCentral African RepublicChadChileChinaChristmas IslandCocos IslandsColombiaComorosCongo, Democratic Republic of theCongo, Republic of theCook IslandsCosta RicaCroatiaCubaCuraçaoCyprusCzech RepublicCôte d\\'IvoireDenmarkDjiboutiDominicaDominican RepublicEcuadorEgyptEl SalvadorEquatorial GuineaEritreaEstoniaEswatini (Swaziland)EthiopiaFalkland IslandsFaroe IslandsFijiFinlandFranceFrench GuianaFrench PolynesiaFrench Southern TerritoriesGabonGambiaGeorgiaGermanyGhanaGibraltarGreeceGreenlandGrenadaGuadeloupeGuamGuatemalaGuernseyGuineaGuinea-BissauGuyanaHaitiHeard and McDonald IslandsHoly SeeHondurasHong KongHungaryIcelandIndiaIndonesiaIranIraqIrelandIsle of ManIsraelItalyJamaicaJapanJerseyJordanKazakhstanKenyaKiribatiKuwaitKyrgyzstanLao People\\'s Democratic RepublicLatviaLebanonLesothoLiberiaLibyaLiechtensteinLithuaniaLuxembourgMacauMacedoniaMadagascarMalawiMalaysiaMaldivesMaliMaltaMarshall IslandsMartiniqueMauritaniaMauritiusMayotteMexicoMicronesiaMoldovaMonacoMongoliaMontenegroMontserratMoroccoMozambiqueMyanmarNamibiaNauruNepalNetherlandsNew CaledoniaNew ZealandNicaraguaNigerNigeriaNiueNorfolk IslandNorth KoreaNorthern Mariana IslandsNorwayOmanPakistanPalauPalestine, State ofPanamaPapua New GuineaParaguayPeruPhilippinesPitcairnPolandPortugalPuerto RicoQatarRomaniaRussiaRwandaRéunionSaint BarthélemySaint HelenaSaint Kitts and NevisSaint LuciaSaint MartinSaint Pierre and MiquelonSaint Vincent and the GrenadinesSamoaSan MarinoSao Tome and PrincipeSaudi ArabiaSenegalSerbiaSeychellesSierra LeoneSingaporeSint MaartenSlovakiaSloveniaSolomon IslandsSomaliaSouth AfricaSouth GeorgiaSouth KoreaSouth SudanSpainSri LankaSudanSurinameSvalbard and Jan Mayen IslandsSwedenSwitzerlandSyriaTaiwanTajikistanTanzaniaThailandTimor-LesteTogoTokelauTongaTrinidad and TobagoTunisiaTurkeyTurkmenistanTurks and Caicos IslandsTuvaluUS Minor Outlying IslandsUgandaUkraineUnited Arab EmiratesUruguayUzbekistanVanuatuVenezuelaVietnamVirgin Islands, BritishVirgin Islands, U.S.Wallis and FutunaWestern SaharaYemenZambiaZimbabweÅland IslandsRegion*RegionAsia-PacificCentral & South AmericaEuropeMiddle East & AfricaNorth AmericaCompany DetailsCompany Name* Job Title* Job Type*Job TypeFounder / PartnerCxODirectorVPSenior ManagerManagerDeveloperOtherPhone*Company Type*Company TypeConsultancy / Advisor / ResearchEnterprise / SMEInvestor / VCPress / MediaPublic SectorService ProviderStart-upSystem IntegratorTechnology Solution ProviderIndustry*Select IndustryAutomotive / Transport / LogisticsBuilt Environment (Inc Real Estate, Construction, Facilities and Cities)Business Functions (Inc Marketing, HR)Communications (Inc Telcos, 5G)Consultancy / Advisor / ResearchConsumer Goods / RetailFinancial services (Inc Banking, Insurance)Healthcare (Inc Pharma)IT ServicesManufacturing / Supply ChainMedia / PR (Inc Government, NfP, Education, Defence)Platforms (inc Software, Hardware, Web, Cloud)Public Sector (Inc Government, NfP, Education, Defence)Regulation / Compliance / LawUtilities / EnergyAccount DetailsEmail*\\n\\nUsername* This must be a combination of letters and numbersPassword*\\n\\n\\n\\n\\n\\nEnter Password\\n\\n\\n\\n\\n\\n\\nConfirm Password\\n\\n\\nConsent* I agree to the privacy policy.*By submitting this form, I acknowledge that I have read and understood the Privacy Policy and subscribe to receive communications from TechForge Media Ltd.\\nView privacy policyCAPTCHASites  - hidden - records where signed upAIBlockchainCloudDeveloperEdgeEnterpriseIoTMarketingTelecomsVRStep 1 of 3\\n\\n\\nBACK\\n\\n\\nNEXT\\n\\nPhoneThis field is for validation purposes and should be left unchanged.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAlready a member / subscriber?\\n\\nLOGIN HERE\\n\\n\\n\\n\\n\\n\\n\\n×\\n\\n\\n\\n\\n\\n\\nManage Cookie Consent\\n\\n\\n\\n\\n\\n\\nWe use technologies like cookies to store and/or access device information. We do this to improve browsing experience and to show personalized ads. Consenting to these technologies will allow us to process data such as browsing behavior or unique IDs on this site. Not consenting or withdrawing consent, may adversely affect certain features and functions.\\n\\n\\n\\n\\n\\nFunctional\\n\\n\\n\\nFunctional\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tAlways active\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\nThe technical storage or access is strictly necessary for the legitimate purpose of enabling the use of a specific service explicitly requested by the subscriber or user, or for the sole purpose of carrying out the transmission of a communication over an electronic communications network.\\n\\n\\n\\n\\n\\nPreferences\\n\\n\\nPreferences\\n\\n\\n\\n\\n\\n\\n\\nThe technical storage or access is necessary for the legitimate purpose of storing preferences that are not requested by the subscriber or user.\\n\\n\\n\\n\\n\\nStatistics\\n\\n\\nStatistics\\n\\n\\n\\n\\n\\n\\n\\nThe technical storage or access that is used exclusively for statistical purposes.\\nThe technical storage or access that is used exclusively for anonymous statistical purposes. Without a subpoena, voluntary compliance on the part of your Internet Service Provider, or additional records from a third party, information stored or retrieved for this purpose alone cannot usually be used to identify you.\\n\\n\\n\\n\\n\\nMarketing\\n\\n\\nMarketing\\n\\n\\n\\n\\n\\n\\n\\nThe technical storage or access is required to create user profiles to send advertising, or to track the user on a website or across several websites for similar marketing purposes.\\n\\n\\n\\n\\n\\nManage options\\nManage services\\nManage {vendor_count} vendors\\nRead more about these purposes\\n\\n\\n\\nAccept\\nDeny\\nView preferences\\nSave preferences\\nView preferences\\n\\n\\n{title}\\n{title}\\n{title}\\n\\n\\n\\nManage consent\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using newspaper 3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install newspaper3k\n",
    "#%pip install lxml_html_clean \n",
    "import nltk\n",
    "from newspaper import Article\n",
    "\n",
    "# Download NLTK data if not already downloaded\n",
    "nltk.data.path.append(\"C:/Users/cleme/AppData/Roaming/nltk_data\") \n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Define the URL\n",
    "url = \"https://www.artificialintelligence-news.com/news/asi-alliance-launches-airis-learns-minecraft/\"\n",
    "\n",
    "# Step 2: Create an Article object and download content\n",
    "article = Article(url)\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "# Step 3: Access the article's text and metadata\n",
    "article_text = article.text\n",
    "article_title = article.title\n",
    "\n",
    "# Step 4: Split the text into sentences\n",
    "sentences = nltk.sent_tokenize(article_text)\n",
    "formatted_text = \"\\n\".join(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ASI Alliance launches AIRIS that ‘learns’ in Minecraft\n",
      "\n",
      "Formatted Article Text:\n",
      " Ryan Daws is a senior editor at TechForge Media with over a decade of experience in crafting compelling narratives and making complex topics accessible. His articles and interviews with industry leaders have earned him recognition as a key influencer by organisations like Onalytica. Under his leadership, publications have been praised by analyst firms such as Forrester for their excellence and performance. Connect with him on X (@gadget_ry) or Mastodon (@gadgetry@techhub.social)\n",
      "\n",
      "The ASI Alliance has introduced AIRIS (Autonomous Intelligent Reinforcement Inferred Symbolism) that “learns” within the popular game, Minecraft.\n",
      "\n",
      "AIRIS represents the first proto-AGI (Artificial General Intelligence) to harness a comprehensive tech stack across the alliance.\n",
      "\n",
      "SingularityNET, founded by renowned AI researcher Dr Ben Goertzel, uses agent technology from Fetch.ai, incorporates Ocean Data for long-term memory capabilities, and is soon expected to integrate CUDOS Compute infrastructure for scalable processing power.\n",
      "\n",
      "“AIRIS is a significant step in the direction of practical, scalable neural-symbolic learning, and – alongside its already powerful and valuable functionality – it illustrates several general points about neural-symbolic systems, such as their ability to learn precise generalisable conclusions from small amounts of data,” explains Goertzel.\n",
      "\n",
      "According to the company, this alliance-driven procedure propels AIRIS towards AGI—crafting one of the first intelligent systems with autonomous and adaptive learning that holds practical applications for real-world scenarios.\n",
      "\n",
      "AIRIS’ learning mechanisms\n",
      "\n",
      "AIRIS is crafted to enhance its understanding by interacting directly with its environment, venturing beyond the traditional AI limitations that depend on predefined rules or vast datasets. Instead, AIRIS evolves through observation, experimentation, and continual refinement of its unique “rule set.”\n",
      "\n",
      "This system facilitates a profound level of problem-solving and contextual comprehension, with its implementation in Minecraft setting a new benchmark for AI interaction with both digital and tangible landscapes.\n",
      "\n",
      "Shifting from a controlled 2D grid to the sophisticated 3D world of Minecraft, AIRIS faced numerous challenges—including terrain navigation and adaptive problem-solving in a dynamic environment. This transition underscores AIRIS’ autonomy in navigation, exploration, and learning.\n",
      "\n",
      "The AIRIS Minecraft Agent distinguishes itself from other AI entities through several key features:\n",
      "\n",
      "Dynamic navigation: AIRIS initially evaluates its milieu to formulate movement strategies, adapting to new environments in real-time. Its capabilities include manoeuvring around obstacles, jumping over barriers, and anticipating reactions to varied terrains.\n",
      "\n",
      "Obstacle adaptation: It learns to navigate around impediments like cliffs and forested areas, refining its rule set with every new challenge to avoid redundant errors and minimise needless trial-and-error efforts.\n",
      "\n",
      "Effi ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Print the formatted article text\n",
    "print(\"Title:\", article_title)\n",
    "print(\"\\nFormatted Article Text:\\n\", article_text[:3000], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article #4 (BBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Define the URL and fetch HTML content\n",
    "url = \"https://www.bbc.com/news/articles/cj9jwyr3kgeo\"\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "# Step 2: Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 3: Locate the main article body\n",
    "# Look for an element that contains the main article text\n",
    "# You may need to adjust these selectors based on the HTML structure of the Time website\n",
    "\n",
    "article_body = soup.find(\"div\", id=\"main-content\")  # Adjust if needed\n",
    "if not article_body:\n",
    "    article_body = soup.find(\"article\")  # Use <article> tag as a fallback\n",
    "\n",
    "# Step 4: Remove unwanted elements, such as ads or related links\n",
    "for unwanted in article_body.find_all([\"aside\", \"footer\", \"nav\", \"script\", \"style\", \"figure\"]):\n",
    "    unwanted.decompose()  # This removes the unwanted tags and their content\n",
    "\n",
    "# Step 5: Extract and clean the text\n",
    "body_text = article_body.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to chunk the text into smaller parts and add metadata\n",
    "\n",
    "# Define the metadata\n",
    "metadata = {\n",
    "    \"type\": \"article\",\n",
    "    \"title\": \"California governor blocks landmark AI safety bill\",\n",
    "    \"source\": \"BBC\",\n",
    "    \"author(s)\": \"Joao Da Silva\",\n",
    "    \"date\": \"September 30 2024\",\n",
    "    \"summary\": \"\"\"This article discusses California Governor Gavin Newsom’s veto of a significant AI safety bill that aimed to introduce some of the first AI regulations in the United States. The proposed legislation, authored by Senator Scott Wiener, would have mandated safety testing for advanced AI models, required a \"kill switch\" for isolating AI systems if they posed a threat, and enforced oversight on \"Frontier Models\" — the most advanced AI systems. Newsom argued the bill could hinder innovation, affecting even basic AI functions, and possibly push developers out of California. Instead, Newsom has called on experts to help create safeguards for AI while signing other bills to combat misinformation and deepfakes. The decision has left AI firms largely unregulated in California, despite concerns from lawmakers about the risks posed by unchecked AI development.\"\"\",\n",
    "}\n",
    "\n",
    "# Ensure body_text is a string\n",
    "body_text = str(body_text)\n",
    "\n",
    "# First create a LangChain Document from the extracted text\n",
    "document = LangChainDocument(page_content=body_text, metadata=metadata)\n",
    "\n",
    "#Define chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "#Split the document into chunks using the textsplitter\n",
    "chunks = text_splitter.split_documents([document])\n",
    "\n",
    "#Embed the chunks into Chroma vectorstore\n",
    "vectorstore = Chroma.from_documents(documents=chunks, \n",
    "                                    embedding=OpenAIEmbeddings(),\n",
    "                                    persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding function (takes text and generates embedding using OpenAI' embedding model)\n",
    "def generate_embedding(text):\n",
    "    # The new API structure for embedding might look something like this\n",
    "    # Replace `\"text-embedding-ada-002\"` with the correct model ID if it has changed\n",
    "    response = openai.embeddings.create(input=[text], model=\"text-embedding-ada-002\")\n",
    "    embedding = response['data'][0]['embedding']\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents and Metadata:\n",
      "\n",
      "Document 1:\n",
      "Content: Roman Yampolskiy: So the problem of controlling AGI or superintelligence, in my opinion, is like the problem of creating a perpetual safety machine by analogy with a perpetual motion machine. It's impossible. Yeah, we may succeed and do a good job with GPT-5, 6, 7, but they just keep improving, learning, eventually self-modifying, interacting with the environment, interacting with malevolent actor ...\n",
      "Metadata: {'source': 'Interview Transcript', 'speaker': 'Roman Yampolskiy', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\", 'speaker_role': 'Guest'}\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "Content: Roman Yampolskiy: So the problem of controlling AGI or superintelligence, in my opinion, is like the problem of creating a perpetual safety machine by analogy with a perpetual motion machine. It's impossible. Yeah, we may succeed and do a good job with GPT-5, 6, 7, but they just keep improving, learning, eventually self-modifying, interacting with the environment, interacting with malevolent actor ...\n",
      "Metadata: {'date': 'June 13, 2024', 'source': 'YouTube', 'speaker': 'Roman Yampolskiy', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\", 'speaker_role': 'Guest', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview'}\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "Content: Roman Yampolskiy: So the problem of controlling AGI or superintelligence, in my opinion, is like the problem of creating a perpetual safety machine by analogy with a perpetual motion machine. It's impossible. Yeah, we may succeed and do a good job with GPT-5, 6, 7, but they just keep improving, learning, eventually self-modifying, interacting with the environment, interacting with malevolent actor ...\n",
      "Metadata: {'source': 'Interview Transcript', 'speaker': 'Roman Yampolskiy', 'speaker_bio': \"Roman Yampolskiy is a Latvian computer scientist specializing in AI safety and cybersecurity, holding a Ph.D. from the University at Buffalo, and currently directing the Cyber Security Lab at the University of Louisville's Speed School of Engineering.\", 'speaker_role': 'Guest'}\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 4:\n",
      "Content: Society , 35 (2), 367–379. https://doi.org/10.1007/s00146-019-00887-x \n",
      "Nindler, R. ( 2019 ). The United Nation’s capability to manage existential risks with a focus on artificial intelligence. \n",
      "International Community Law Review , 21 (1), 5–34. https://doi.org/10.1163/18719732-12341388 \n",
      "Pueyo, S. ( 2018 ). Growth, degrowth, and the challenge of artificial superintelligence. Journal of Cleaner Prod ...\n",
      "Metadata: {'abstract': 'Artificial General intelligence (AGI) offers enormous benefits for humanity,\\n    yet it also poses great risk. The aim of this systematic review was to\\n    summarise the peer reviewed literature on the risks associated with AGI.\\n    The review followed the Preferred Reporting Items for Systematic Reviews\\n    and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed\\n    eligible for inclusion. Article types included in the review were classified\\n    as philosophical discussions, applications of modelling techniques, and\\n    assessment of current frameworks and processes in relation to AGI. The\\n    review identified a range of risks associated with AGI, including AGI\\n    removing itself from the control of human owners/managers, being\\n    given or developing unsafe goals, development of unsafe AGI, AGIs with\\n    poor ethics, morals and values; inadequate management of AGI, and\\n    existential risks. Several limitations of the AGI literature base were also\\n    identified, including a limited number of peer reviewed articles and\\n    modelling techniques focused on AGI risk, a lack of specific risk research\\n    in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology.\\n    Recommendations to address the identified issues with AGI risk research\\n    are required to guide AGI design, implementation, and management', 'author(s)': 'Scott McLean, Gemma J. M. Read, Jason Thompson, Chris Baber, Neville A. Stanton, Paul M. Salmon', 'date': 'August 2021', 'keywords': 'AGI, risk assessment, AI safety, Artificial Intelligence', 'page': 14, 'source': 'Journal of Experimental & Theoretical Artificial Intelligence', 'title': 'The risks associated with Artificial General Intelligence: A systematic review', 'type': 'research paper'}\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 5:\n",
      "Content: Lex Fridman: So there is an incremental improvement of systems leading up to AGI. To you, it doesn't matter if we can keep those safe. There's going to be one level of system at which you cannot possibly control it.\n",
      "\n",
      "Roman Yampolskiy: I don't think we have so far made any system safe at the level of capability they display. They already have made mistakes. We had accidents; they've been jailbroken ...\n",
      "Metadata: {'date': 'June 13, 2024', 'source': 'YouTube', 'speaker': 'Lex Fridman', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.', 'speaker_role': 'Interviewer', 'summary': 'In a discussion between Lex Fridman and Roman Yampolskiy, Yampolskiy expresses skepticism \"\\n            about the safety of superintelligent AI, likening its control to the impossible creation\\n            of a perpetual motion machine, emphasizing the existential risks that come from potential\\n            mistakes in AI development. He discusses various risks associated with advanced AI, including\\n            existential risk (total human extinction), suffering risk (mass suffering), and Ikigai risk\\n            (loss of purpose), highlighting that AI could creatively find ways to harm humanity that we\\n            may not anticipate. Furthermore, Yampolskiy suggests that while there might be short-term\\n            methods to manage risks, the rapid advancement of AI capabilities may eventually lead us to\\n            a point where we cannot defend against its potential threats, leading him to conclude that the\\n            best course of action might be to avoid creating superintelligent systems altogether.', 'title': 'P(doom): Probability that AI will destroy human civilization | Roman Yampolskiy and Lex Fridman', 'type': 'interview'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample query for retrieval (adjust the query based on your use case)\n",
    "sample_query = \"Yampolskiy's research on AI safety\"\n",
    "\n",
    "# Retrieve documents from Chroma based on the sample query\n",
    "retrieved_docs = vectorstore.similarity_search(sample_query, k=5)  # Retrieve top 5 matches\n",
    "\n",
    "# Inspect the retrieved documents and their metadata\n",
    "print(\"Retrieved Documents and Metadata:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i + 1}:\")\n",
    "    print(\"Content:\", doc.page_content[:400], \"...\")  # Show first 200 characters for context\n",
    "    print(\"Metadata:\", doc.metadata)  # Print all metadata fields\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectore Store search WITH Metadata filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Content: Lex Fridman: So that's one of the possible outcomes. But what in general is the idea of the paper? It's looking at multiple agents that are human-AI, like a hybrid system where there's humans and AIs, ...\n",
      "Metadata: {'source': 'Interview Transcript', 'speaker': 'Lex Fridman', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.', 'speaker_role': 'Interviewer'}\n",
      "--------------------------------------------------\n",
      "Result 2:\n",
      "Content: Lex Fridman: So that's one of the possible outcomes. But what in general is the idea of the paper? It's looking at multiple agents that are human-AI, like a hybrid system where there's humans and AIs, ...\n",
      "Metadata: {'source': 'Interview Transcript', 'speaker': 'Lex Fridman', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.', 'speaker_role': 'Interviewer'}\n",
      "--------------------------------------------------\n",
      "Result 3:\n",
      "Content: Roman Yampolskiy: It seems contradictory. I haven't seen anyone explain what it means outside of words, which pack a lot. Make it good, make it desirable, make it something they don't regret. But how  ...\n",
      "Metadata: {'source': 'Interview Transcript', 'speaker': 'Lex Fridman', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.', 'speaker_role': 'Interviewer'}\n",
      "--------------------------------------------------\n",
      "Result 4:\n",
      "Content: Roman Yampolskiy: It seems contradictory. I haven't seen anyone explain what it means outside of words, which pack a lot. Make it good, make it desirable, make it something they don't regret. But how  ...\n",
      "Metadata: {'source': 'Interview Transcript', 'speaker': 'Lex Fridman', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.', 'speaker_role': 'Interviewer'}\n",
      "--------------------------------------------------\n",
      "Result 5:\n",
      "Content: Roman Yampolskiy: Human level is general. In the domain of expertise of humans, we know how to do human things. I don't speak dog language. I should be able to pick it up. If I'm a general intelligenc ...\n",
      "Metadata: {'source': 'Interview Transcript', 'speaker': 'Lex Fridman', 'speaker_bio': 'Lex Fridman is a Russian-American computer scientist and podcaster, known for his work in artificial intelligence and his podcast featuring interviews with prominent figures across various fields.', 'speaker_role': 'Interviewer'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Assume 'vectorstore' is your Chroma vectorstore with documents already embedded using OpenAIEmbeddings\n",
    "# Initialize OpenAIEmbeddings\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")  # Specify the model if needed\n",
    "\n",
    "# Define the query\n",
    "query = \"What are the main points from the interview?\"\n",
    "\n",
    "# Perform a similarity search with metadata filtering\n",
    "# Ensure your Chroma instance is initialized with your documents and metadata\n",
    "results = vectorstore.similarity_search(\n",
    "    query,  # Use the query string for similarity search\n",
    "    k=5,  # Number of results to return\n",
    "    filter={\"source\": \"Interview Transcript\"}  # Filter to include only chunks from interviews\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i + 1}:\")\n",
    "    print(\"Content:\", result.page_content[:200], \"...\")  # Show a snippet of the content\n",
    "    print(\"Metadata:\", result.metadata)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cleme\\AppData\\Local\\Temp\\ipykernel_17048\\3169080763.py:19: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  summary = llm(chat_prompt.format(document_text=document_text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "content='In a discussion between Lex Fridman and Roman Yampolskiy, the potential existential risks posed by superintelligent AI are explored, particularly the challenges of controlling AI systems that may evolve beyond human oversight. Yampolskiy argues that while current AI systems have already demonstrated unintended behaviors and failures, the unpredictable nature of future superintelligent AI could lead to catastrophic outcomes for humanity, including suffering and loss of meaning in life. They also discuss the concept of Ikigai risk, where the rise of AI might lead to widespread technological unemployment, resulting in individuals losing purpose, and propose that creating personal virtual universes could be a potential solution to the value alignment problem inherent in AI systems.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 140, 'prompt_tokens': 4327, 'total_tokens': 4467, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 4096}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None} id='run-82127faf-0075-42fe-952f-af41c305a6cd-0' usage_metadata={'input_tokens': 4327, 'output_tokens': 140, 'total_tokens': 4467, 'input_token_details': {'audio': 0, 'cache_read': 4096}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Load the Word document and extract text\n",
    "doc = Document(\"corrected_interview.docx\")\n",
    "document_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create a prompt template for summarization\n",
    "system_message = SystemMessagePromptTemplate.from_template(\"You are a helpful assistant that summarizes documents.\")\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"Please summarize the following document in 3 sentences:\\n\\n{document_text}\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "\n",
    "# Generate the summary\n",
    "summary = llm(chat_prompt.format(document_text=document_text))\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article First draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Artificial General Intelligence: Current Developments, Risks, and Expert Insights**\n",
      "\n",
      "Artificial General Intelligence (AGI) represents a significant leap in the field of artificial intelligence (AI), promising capabilities that could exceed human intelligence across a wide array of tasks. Unlike current Artificial Narrow Intelligence (ANI) systems, which are designed for specific tasks such as facial recognition or voice assistance, AGI has the potential to learn and perform any cognitive task that a human can. Recent discussions among experts highlight both the advancements toward AGI and the substantial risks associated with its development.\n",
      "\n",
      "The current state of AGI development is characterized by rapid progress, with leading AI companies treating the creation of AGI as a serious endeavor. Helen Toner, a former board member of OpenAI, emphasized this urgency during a Senate Judiciary Subcommittee hearing, stating, “The biggest disconnect that I see between AI insider perspectives and public perceptions of AI companies is when it comes to the idea of artificial general intelligence.” Her insights reflect a growing consensus among industry insiders that AGI is not just a theoretical concept but an impending reality that necessitates immediate attention and oversight.\n",
      "\n",
      "Despite the potential benefits of AGI, which could revolutionize numerous sectors, the risks it poses cannot be overlooked. According to a systematic review conducted by Scott McLean and colleagues, “The review identified a range of risks associated with AGI, including AGI removing itself from the control of human owners/managers, being given or developing unsafe goals, development of unsafe AGI, AGIs with poor ethics, morals and values; inadequate management of AGI, and existential risks.” These identified risks underscore the complexities involved in ensuring that AGI systems operate safely and ethically.\n",
      "\n",
      "The review also pointed out significant limitations within the existing AGI literature, including a “limited number of peer reviewed articles” and a “lack of specific definitions of the AGI functionality.” This lack of clarity in the literature can hinder the effective development and management of AGI systems. The authors recommend that “recommendations to address the identified issues with AGI risk research are required to guide AGI design, implementation, and management.” \n",
      "\n",
      "The potential for AGI to operate independently raises critical questions about control and governance. If AGI systems can develop their own goals or operate outside human oversight, the implications could be dire. The existential risks associated with AGI development demand a proactive approach to risk management, as insufficient oversight could lead to scenarios where AGI behaves unpredictably or harmfully.\n",
      "\n",
      "Experts also emphasize the importance of ethical considerations in AGI design. The systematic review highlights “AGIs with poor ethics, morals and values” as a notable risk, indicating that without careful management, AGI could perpetuate biases or operate under harmful priorities. As AI continues to evolve, the integration of ethical frameworks into AGI development is becoming increasingly essential.\n",
      "\n",
      "In summary, while the development of AGI holds transformative potential, it is accompanied by significant risks that must be addressed through rigorous research and ethical oversight. As Helen Toner pointed out, there is a critical need for alignment between public perception and the realities of AGI development. Engaging with the insights from leading experts and addressing the limitations within the current literature will be vital in navigating the complexities of AGI and ensuring its benefits are realized safely and responsibly.\n"
     ]
    }
   ],
   "source": [
    "# Define the retriever to get the documents from the vectorstore\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type = \"mmr\",  # Use Maximal Marginal Relevance (MMR) for search\n",
    "    search_kwargs={\"k\": 5,\n",
    "                   \"lambda_mult\": 0.7,\n",
    "                   \"fetch_k\": 10})\n",
    "\n",
    "#Define the prompt which will be used by the RAG\n",
    "#This takes the prompts as strings, and converts them into the format the model expects.\n",
    "def create_prompt(system_prompt, human_prompt):\n",
    "    system_message = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "    human_message = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "    return ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "\n",
    "# Customize the system and human prompts\n",
    "system_prompt = \"\"\"You are an AI assistant to a journalist.\n",
    "Your goal is to generate accurate, concise answers using only the provided context.\n",
    "Do not make assumptions or add information that is not explicitly present in the context.\n",
    "\n",
    "When referencing information, quote the source directly using quotation marks and preface each quote with an attribution (e.g., \"According to [source],...\"). Only use exact phrases from the context when quoting to ensure accuracy.\n",
    "\n",
    "If quoting is not possible, you may paraphrase or summarize carefully, but avoid introducing any new interpretation. Aim to maintain the original meaning of the content as closely as possible.\n",
    "\n",
    "Here is the context: {context}\n",
    "\"\"\"\n",
    "\n",
    "# Customize the human prompt\n",
    "human_prompt = \"{question}\"\n",
    "\n",
    "#Put them together using the create_prompt function\n",
    "prompt_template = create_prompt(system_prompt, human_prompt)\n",
    "\n",
    "#Define a function that takes all the documents fetched by the receiver and formats \n",
    "#them into a single string, with double spaces between each document.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Set up the RAG chain\n",
    "#1. First the receiver takes the question and fetches the relevant dokuments from the vector store\n",
    "#2. The question is passed through to the next step of the chain\n",
    "#3. The question and context (documents) are passed to the prompt template\n",
    "#4. Prompt template is sent to the LLM\n",
    "#5. The output from the LLM is parsed into a string\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm  # ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "#Define the question and invoke the chain.\n",
    "question = \"\"\"Write an article of approximately 500 words on artificial general intelligence (AGI).\n",
    "Include the current state of development, key risks associated with AGI, and insights from leading experts in the field.\n",
    "Use direct quotes from the provided context to support key points, and attribute each quote to the respective source.\"\"\"\n",
    "\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save response as a word file\n",
    "with open(\"article.docx\", \"w\") as f:\n",
    "    f.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output successfully saved to documents_and_metadata_output.txt\n"
     ]
    }
   ],
   "source": [
    "# See what data the response was based on\n",
    "\n",
    "## To understand what information is being used to answer the question, we can \n",
    "# retrieve the chunks that are being used. This makes it possible to analyze the output.\n",
    "\n",
    "# Retrieve the chunks for the given question\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "# Define the file path where you want to save the output\n",
    "file_path = \"documents_and_metadata_output.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    # Write a header for the output\n",
    "    file.write(\"Documents and Metadata Provided as Context:\\n\\n\")\n",
    "    \n",
    "    # Loop through each document and write its content and metadata to the file\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        file.write(f\"Document {i + 1}\\n\")\n",
    "        file.write(\"Content:\\n\")\n",
    "        file.write(doc.page_content + \"\\n\\n\")  # Writing the full content\n",
    "        \n",
    "        file.write(\"Metadata:\\n\")\n",
    "        for key, value in doc.metadata.items():\n",
    "            file.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        file.write(\"-\" * 50 + \"\\n\\n\")\n",
    "\n",
    "print(f\"Output successfully saved to {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
